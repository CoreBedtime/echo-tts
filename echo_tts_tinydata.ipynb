{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Echo-TTS Rap Style Fine-Tuning\n",
    "\n",
    "This notebook implements LoRA-based fine-tuning for Echo-TTS to adapt the model to **rapping style** while preserving voice cloning capabilities.\n",
    "\n",
    "**Dataset:**\n",
    "- 225 rap acapellas (unprocessed)\n",
    "- Requires preprocessing: segmentation, transcription\n",
    "\n",
    "**Requirements:**\n",
    "- GPU with 16GB+ VRAM (T4/A100 on Colab)\n",
    "\n",
    "**What you'll get:**\n",
    "- Fine-tuned model that generates rap-style speech\n",
    "- LoRA checkpoint (~50-100MB) that can be loaded on top of base model\n",
    "- Preserved voice cloning - can still use any speaker reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hWorking directory: /content/echo-tts\n"
     ]
    }
   ],
   "source": [
    "# Clone the echo-tts repository\n",
    "!rm -rf /content/echo-tts\n",
    "!git clone https://github.com/CoreBedtime/echo-tts.git 2>/dev/null || echo \"Repo already exists\"\n",
    "\n",
    "# Add to Python path\n",
    "import sys\n",
    "sys.path.insert(0, '/content/echo-tts')\n",
    "\n",
    "# Change working directory\n",
    "import os\n",
    "os.chdir('/content/echo-tts')\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q torch torchaudio safetensors huggingface-hub einops\n",
    "!pip install -q openai-whisper  # For automatic transcription\n",
    "!pip install -q torchcodec  # For audio decoding\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.1)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (0.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchaudio safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for saving checkpoints (Colab)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu126\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "VRAM: 42.5 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio directory: /content/dataset/\n",
      "Output directory: ./checkpoints/\n",
      "Device: cuda\n",
      "\n",
      "Training config for 225 samples:\n",
      "  LoRA rank: 32\n",
      "  Learning rate: 0.0001\n",
      "  Epochs: 10\n",
      "  Effective batch size: 8\n",
      "\n",
      "Transcription config:\n",
      "  Whisper model: large-v3\n",
      "  Parallel workers: 4\n",
      "  This will be MUCH faster than sequential!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# =============================================================================\n",
    "# PATHS - Update these!\n",
    "# =============================================================================\n",
    "\n",
    "# Directory containing your training audio files\n",
    "AUDIO_DIR = \"/content/dataset/\"  # Put your rap acapellas here\n",
    "\n",
    "# Where to save checkpoints\n",
    "OUTPUT_DIR = \"./checkpoints/\" \n",
    "\n",
    "# =============================================================================\n",
    "# LoRA CONFIGURATION (optimized for 225 samples)\n",
    "# =============================================================================\n",
    "\n",
    "LORA_RANK = 32          # Higher rank for larger dataset (more expressiveness)\n",
    "LORA_ALPHA = 32.0       # Scaling factor, typically equal to rank\n",
    "LORA_DROPOUT = 0.05     # Lower dropout - more data means less regularization needed\n",
    "\n",
    "# Which modules to train (default preserves voice cloning path)\n",
    "# Speaker path (wk_speaker, wv_speaker) is NOT trained to preserve cloning\n",
    "TARGET_MODULES = [\n",
    "    # Main decoder attention (style)\n",
    "    \"blocks.*.attention.wq\",\n",
    "    \"blocks.*.attention.wk\",\n",
    "    \"blocks.*.attention.wv\",\n",
    "    \"blocks.*.attention.wo\",\n",
    "    # Text cross-attention (text-to-audio mapping)\n",
    "    \"blocks.*.attention.wk_text\",\n",
    "    \"blocks.*.attention.wv_text\",\n",
    "    # MLP layers (feature transformation)\n",
    "    \"blocks.*.mlp.w1\",\n",
    "    \"blocks.*.mlp.w2\",\n",
    "    \"blocks.*.mlp.w3\",\n",
    "]\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING CONFIGURATION (optimized for 225 samples)\n",
    "# =============================================================================\n",
    "\n",
    "LEARNING_RATE = 1e-4     # Slightly higher LR for larger dataset\n",
    "NUM_EPOCHS = 10          # Fewer epochs needed with more data\n",
    "BATCH_SIZE = 1           # Batch size (1 for memory efficiency)\n",
    "GRADIENT_ACCUMULATION = 8  # Larger effective batch for stability\n",
    "MAX_GRAD_NORM = 1.0      # Gradient clipping\n",
    "WARMUP_STEPS = 100       # More warmup steps for larger dataset\n",
    "\n",
    "# Audio settings\n",
    "MAX_LATENT_LENGTH = 640  # Max ~30 seconds (reduce to 320 if OOM)\n",
    "SEGMENT_DURATION = 25.0  # Split long audio into ~25 second chunks\n",
    "MIN_SEGMENT_DURATION = 5.0  # Minimum segment length to keep\n",
    "\n",
    "# Whisper model for transcription - LARGE-V3 for best rap lyrics accuracy!\n",
    "WHISPER_MODEL = \"large-v3\"  # Options: tiny, base, small, medium, large-v3\n",
    "\n",
    "# Parallel transcription settings\n",
    "NUM_TRANSCRIPTION_WORKERS = 4  # Number of parallel workers (adjust based on CPU/GPU)\n",
    "TRANSCRIPTION_BATCH_SIZE = 8   # Files per batch for progress updates\n",
    "\n",
    "# Device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.bfloat16   # Use bfloat16 for training\n",
    "\n",
    "# Validation split\n",
    "VAL_SPLIT = 0.1  # 10% for validation (~22 samples)\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(AUDIO_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Audio directory: {AUDIO_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"\\nTraining config for 225 samples:\")\n",
    "print(f\"  LoRA rank: {LORA_RANK}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"\\nTranscription config:\")\n",
    "print(f\"  Whisper model: {WHISPER_MODEL}\")\n",
    "print(f\"  Parallel workers: {NUM_TRANSCRIPTION_WORKERS}\")\n",
    "print(f\"  This will be MUCH faster than sequential!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EchoDiT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8b688ac4a34b2db0671bf3f18e6009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.safetensors:   0%|          | 0.00/5.60G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: 2800.7M parameters\n",
      "\n",
      "Loading Fish-S1-DAC autoencoder...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea1d75681434c95822a50d430676140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.safetensors:   0%|          | 0.00/1.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder loaded\n",
      "\n",
      "Loading PCA state...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3e4b6c980a4dfc80e02656186eb069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pca_state.safetensors:   0%|          | 0.00/332k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA state loaded\n",
      "\n",
      "==================================================\n",
      "All models loaded successfully!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from inference import (\n",
    "    load_model_from_hf,\n",
    "    load_fish_ae_from_hf,\n",
    "    load_pca_state_from_hf,\n",
    "    ae_decode,\n",
    "    get_speaker_latent_and_mask,\n",
    "    get_text_input_ids_and_mask,\n",
    "    sample_euler_cfg_independent_guidances,\n",
    "    sample_pipeline,\n",
    "    load_audio,\n",
    ")\n",
    "from functools import partial\n",
    "\n",
    "print(\"Loading EchoDiT model...\")\n",
    "model = load_model_from_hf(\n",
    "    device=DEVICE,\n",
    "    dtype=DTYPE,\n",
    "    compile=False,  # Don't compile for training\n",
    "    delete_blockwise_modules=True,  # Save memory\n",
    ")\n",
    "print(f\"Model loaded: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M parameters\")\n",
    "\n",
    "print(\"\\nLoading Fish-S1-DAC autoencoder...\")\n",
    "fish_ae = load_fish_ae_from_hf(\n",
    "    device=DEVICE,\n",
    "    dtype=torch.float32,  # AE needs float32 for quality\n",
    ")\n",
    "print(\"Autoencoder loaded\")\n",
    "\n",
    "print(\"\\nLoading PCA state...\")\n",
    "pca_state = load_pca_state_from_hf(device=DEVICE)\n",
    "print(\"PCA state loaded\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"All models loaded successfully!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation\n",
    "\n",
    "With 225 rap acapellas, we need to:\n",
    "1. List and validate all audio files\n",
    "2. Segment long tracks into training-sized chunks (~25 seconds)\n",
    "3. Transcribe using Whisper (medium model for better rap lyrics)\n",
    "4. Create train/validation split\n",
    "5. Pre-encode all audio to latents (cached for fast training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 173 audio files\n",
      "==================================================\n",
      "\n",
      "Dataset Statistics:\n",
      "  Total files: 173\n",
      "  Total duration: 33347.5s (555.8 min, 9.26 hours)\n",
      "  Average duration: 192.8s\n",
      "  Shortest: 59.4s\n",
      "  Longest: 415.4s\n",
      "\n",
      "Duration distribution:\n",
      "  30-60s: 1 files\n",
      "  60-120s: 24 files\n",
      "  120-180s: 45 files\n",
      "  180-300s: 96 files\n",
      "  >300s: 7 files\n"
     ]
    }
   ],
   "source": [
    "# List and analyze all audio files\n",
    "from pathlib import Path\n",
    "import random\n",
    "import torchaudio\n",
    "import torchcodec\n",
    "\n",
    "AUDIO_EXTENSIONS = (\".mp3\", \".wav\", \".flac\", \".ogg\", \".m4a\")\n",
    "\n",
    "audio_dir = Path(AUDIO_DIR)\n",
    "audio_files = []\n",
    "for ext in AUDIO_EXTENSIONS:\n",
    "    audio_files.extend(audio_dir.glob(f\"*{ext}\"))\n",
    "    audio_files.extend(audio_dir.glob(f\"*{ext.upper()}\"))\n",
    "    # Also check subdirectories\n",
    "    audio_files.extend(audio_dir.glob(f\"**/*{ext}\"))\n",
    "    audio_files.extend(audio_dir.glob(f\"**/*{ext.upper()}\"))\n",
    "\n",
    "# Remove duplicates and sort\n",
    "audio_files = sorted(set(audio_files))\n",
    "\n",
    "print(f\"Found {len(audio_files)} audio files\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Analyze durations\n",
    "durations = []\n",
    "valid_files = []\n",
    "for f in audio_files:\n",
    "    try:\n",
    "        waveform, sample_rate = torchaudio.load(str(f))\n",
    "        duration = waveform.shape[1] / sample_rate\n",
    "        durations.append(duration)\n",
    "        valid_files.append(f)\n",
    "    except Exception as e:\n",
    "        print(f\"  Skipping {f.name}: {e}\")\n",
    "\n",
    "audio_files = valid_files\n",
    "total_duration = sum(durations)\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Total files: {len(audio_files)}\")\n",
    "print(f\"  Total duration: {total_duration:.1f}s ({total_duration/60:.1f} min, {total_duration/3600:.2f} hours)\")\n",
    "print(f\"  Average duration: {sum(durations)/len(durations):.1f}s\")\n",
    "print(f\"  Shortest: {min(durations):.1f}s\")\n",
    "print(f\"  Longest: {max(durations):.1f}s\")\n",
    "\n",
    "# Show duration distribution\n",
    "print(f\"\\nDuration distribution:\")\n",
    "for bucket in [(0, 30), (30, 60), (60, 120), (120, 180), (180, 300), (300, float('inf'))]:\n",
    "    count = sum(1 for d in durations if bucket[0] <= d < bucket[1])\n",
    "    if count > 0:\n",
    "        label = f\"{bucket[0]}-{bucket[1]}s\" if bucket[1] != float('inf') else f\">{bucket[0]}s\"\n",
    "        print(f\"  {label}: {count} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PARALLEL TRANSCRIPTION with Whisper large-v3\n",
      "============================================================\n",
      "Files to transcribe: 173\n",
      "Workers: 4\n",
      "\n",
      "Estimated time:\n",
      "  Sequential (old): ~86 minutes\n",
      "  Parallel (new): ~22 minutes\n",
      "  Speedup: 4x faster!\n",
      "\n",
      "Transcribing 173 files with Whisper 'large-v3'...\n",
      "Using 4 parallel workers\n",
      "This will be MUCH faster than sequential processing!\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't get local object 'transcribe_audio_files_parallel.<locals>.transcribe_single'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2142588268.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Use parallel transcription (MUCH faster!)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     batch_transcriptions = transcribe_audio_files_parallel(\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0maudio_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles_to_transcribe\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWHISPER_MODEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/echo-tts/train_utils.py\u001b[0m in \u001b[0;36mtranscribe_audio_files_parallel\u001b[0;34m(audio_paths, model_name, language, num_workers, batch_size)\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[0;31m# Process in batches for progress updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_start\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m             \u001b[0mbatch_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_start\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         '''\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_handle_tasks\u001b[0;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[1;32m    538\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m                         \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m                         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/multiprocessing/connection.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get local object 'transcribe_audio_files_parallel.<locals>.transcribe_single'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Transcribe all audio files using Whisper LARGE-V3 in PARALLEL\n",
    "# This will be MUCH faster than sequential processing!\n",
    "\n",
    "from train_utils import transcribe_audio_files_parallel\n",
    "import json\n",
    "\n",
    "TRANSCRIPTION_CACHE = os.path.join(OUTPUT_DIR, \"transcriptions.json\")\n",
    "\n",
    "# Check if we have cached transcriptions\n",
    "if os.path.exists(TRANSCRIPTION_CACHE):\n",
    "    print(f\"Loading cached transcriptions from {TRANSCRIPTION_CACHE}\")\n",
    "    with open(TRANSCRIPTION_CACHE, \"r\") as f:\n",
    "        transcriptions = json.load(f)\n",
    "    print(f\"Loaded {len(transcriptions)} cached transcriptions\")\n",
    "    \n",
    "    # Find files that still need transcription\n",
    "    cached_paths = set(transcriptions.keys())\n",
    "    files_to_transcribe = [f for f in audio_files if str(f) not in cached_paths]\n",
    "    print(f\"Files still needing transcription: {len(files_to_transcribe)}\")\n",
    "else:\n",
    "    transcriptions = {}\n",
    "    files_to_transcribe = audio_files\n",
    "\n",
    "if len(files_to_transcribe) > 0:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PARALLEL TRANSCRIPTION with Whisper {WHISPER_MODEL}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Files to transcribe: {len(files_to_transcribe)}\")\n",
    "    print(f\"Workers: {NUM_TRANSCRIPTION_WORKERS}\")\n",
    "    print(f\"\\nEstimated time:\")\n",
    "    print(f\"  Sequential (old): ~{len(files_to_transcribe) * 0.5:.0f} minutes\")\n",
    "    print(f\"  Parallel (new): ~{len(files_to_transcribe) * 0.5 / NUM_TRANSCRIPTION_WORKERS:.0f} minutes\")\n",
    "    print(f\"  Speedup: {NUM_TRANSCRIPTION_WORKERS}x faster!\\n\")\n",
    "    \n",
    "    # Use parallel transcription (MUCH faster!)\n",
    "    batch_transcriptions = transcribe_audio_files_parallel(\n",
    "        audio_paths=[str(f) for f in files_to_transcribe],\n",
    "        model_name=WHISPER_MODEL,\n",
    "        language=\"en\",\n",
    "        num_workers=NUM_TRANSCRIPTION_WORKERS,\n",
    "        batch_size=TRANSCRIPTION_BATCH_SIZE,\n",
    "    )\n",
    "    \n",
    "    transcriptions.update(batch_transcriptions)\n",
    "    \n",
    "    # Save progress\n",
    "    with open(TRANSCRIPTION_CACHE, \"w\") as f:\n",
    "        json.dump(transcriptions, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nSaved to: {TRANSCRIPTION_CACHE}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Transcription complete: {len(transcriptions)} files\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Show a few samples\n",
    "print(f\"\\nSample transcriptions:\")\n",
    "for i, (path, text) in enumerate(list(transcriptions.items())[:3]):\n",
    "    filename = Path(path).name\n",
    "    print(f\"\\n{filename}:\")\n",
    "    print(f\"  {text[:150]}{'...' if len(text) > 150 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Edit transcriptions manually if Whisper made mistakes\n",
    "# Uncomment and modify as needed:\n",
    "\n",
    "# transcriptions[\"/path/to/file.mp3\"] = \"[S1] Your corrected transcription here.\"\n",
    "\n",
    "# Tips for transcriptions:\n",
    "# - Start with [S1] for single speaker\n",
    "# - Use commas for pauses\n",
    "# - Exclamation marks increase expressiveness\n",
    "# - Keep punctuation natural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 0 training samples (filtered 0 empty)\n",
      "Train samples: 0\n",
      "Validation samples: 0\n",
      "\n",
      "Creating training dataset and encoding audio to latents...\n",
      "This will take a few minutes for 225 files...\n",
      "\n",
      "Pre-encoding audio to latents...\n",
      "Done! Cached 0 latents, 0 speakers\n",
      "\n",
      "Creating validation dataset...\n",
      "Pre-encoding audio to latents...\n",
      "Done! Cached 0 latents, 0 speakers\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1714710088.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# Create dataloaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m train_dataloader = DataLoader(\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    163\u001b[0m                 \u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "# Create training dataset with train/val split\n",
    "from train_utils import (\n",
    "    TrainingSample,\n",
    "    EchoTTSDataset,\n",
    "    collate_fn,\n",
    "    segment_audio,\n",
    "    load_audio_tensor,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "# Create training samples\n",
    "all_samples = []\n",
    "for path, text in transcriptions.items():\n",
    "    if text and len(text.strip()) > 10:  # Filter out empty/very short transcriptions\n",
    "        all_samples.append(TrainingSample(\n",
    "            audio_path=path,\n",
    "            text=text,\n",
    "            speaker_audio_path=None,  # Use same audio as speaker reference\n",
    "        ))\n",
    "\n",
    "print(f\"Created {len(all_samples)} training samples (filtered {len(transcriptions) - len(all_samples)} empty)\")\n",
    "\n",
    "# Shuffle and split into train/val\n",
    "random.seed(42)\n",
    "random.shuffle(all_samples)\n",
    "\n",
    "val_size = max(1, int(len(all_samples) * VAL_SPLIT))\n",
    "train_samples = all_samples[val_size:]\n",
    "val_samples = all_samples[:val_size]\n",
    "\n",
    "print(f\"Train samples: {len(train_samples)}\")\n",
    "print(f\"Validation samples: {len(val_samples)}\")\n",
    "\n",
    "# Create datasets\n",
    "print(\"\\nCreating training dataset and encoding audio to latents...\")\n",
    "print(\"This will take a few minutes for 225 files...\\n\")\n",
    "\n",
    "train_dataset = EchoTTSDataset(\n",
    "    samples=train_samples,\n",
    "    fish_ae=fish_ae,\n",
    "    pca_state=pca_state,\n",
    "    device=DEVICE,\n",
    "    max_latent_length=MAX_LATENT_LENGTH,\n",
    "    cache_latents=True,\n",
    ")\n",
    "\n",
    "print(\"\\nCreating validation dataset...\")\n",
    "val_dataset = EchoTTSDataset(\n",
    "    samples=val_samples,\n",
    "    fish_ae=fish_ae,\n",
    "    pca_state=pca_state,\n",
    "    device=DEVICE,\n",
    "    max_latent_length=MAX_LATENT_LENGTH,\n",
    "    cache_latents=True,\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Dataset ready!\")\n",
    "print(f\"  Train: {len(train_dataset)} samples, {len(train_dataloader)} batches/epoch\")\n",
    "print(f\"  Val: {len(val_dataset)} samples, {len(val_dataloader)} batches\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply LoRA to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lora import (\n",
    "    apply_lora_to_model,\n",
    "    count_parameters,\n",
    "    get_lora_params,\n",
    "    save_lora_checkpoint,\n",
    "    load_lora_checkpoint,\n",
    ")\n",
    "\n",
    "# Apply LoRA adapters to the model\n",
    "print(\"Applying LoRA adapters...\")\n",
    "print(f\"  Rank: {LORA_RANK}\")\n",
    "print(f\"  Alpha: {LORA_ALPHA}\")\n",
    "print(f\"  Dropout: {LORA_DROPOUT}\")\n",
    "print(f\"  Target modules: {len(TARGET_MODULES)} patterns\")\n",
    "\n",
    "model, lora_modules = apply_lora_to_model(\n",
    "    model,\n",
    "    rank=LORA_RANK,\n",
    "    alpha=LORA_ALPHA,\n",
    "    dropout=LORA_DROPOUT,\n",
    "    target_modules=TARGET_MODULES,\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "print(f\"\\nParameter counts:\")\n",
    "print(f\"  Total: {total_params / 1e6:.1f}M\")\n",
    "print(f\"  Trainable (LoRA): {trainable_params / 1e6:.2f}M ({100 * trainable_params / total_params:.2f}%)\")\n",
    "print(f\"  LoRA modules applied: {len(lora_modules)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils import train_epoch, get_cosine_schedule_with_warmup, training_step\n",
    "\n",
    "# Setup optimizer (only LoRA params)\n",
    "lora_params = get_lora_params(model)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    lora_params,\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    betas=(0.9, 0.999),\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "num_training_steps = len(train_dataloader) * NUM_EPOCHS // GRADIENT_ACCUMULATION\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=min(WARMUP_STEPS, num_training_steps // 10),\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "print(\"Training setup:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Training steps: {num_training_steps}\")\n",
    "print(f\"  Warmup steps: {min(WARMUP_STEPS, num_training_steps // 10)}\")\n",
    "print(f\"  Train batches/epoch: {len(train_dataloader)}\")\n",
    "print(f\"  Val batches: {len(val_dataloader)}\")\n",
    "\n",
    "# Validation function\n",
    "@torch.no_grad()\n",
    "def validate(model, val_dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in val_dataloader:\n",
    "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            loss = training_step(model, batch, device)\n",
    "        \n",
    "        # Skip NaN losses in validation too\n",
    "        if not torch.isnan(loss) and not torch.isinf(loss):\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    if num_batches == 0:\n",
    "        return float('nan')\n",
    "    \n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Test a single training step to diagnose NaN issues\n",
    "print(\"Running diagnostic training step...\")\n",
    "\n",
    "# Get a single batch\n",
    "test_batch = next(iter(train_dataloader))\n",
    "\n",
    "# Check batch data\n",
    "print(\"\\nBatch data statistics:\")\n",
    "print(f\"  Latent shape: {test_batch['latent'].shape}\")\n",
    "print(f\"  Latent range: [{test_batch['latent'].min():.4f}, {test_batch['latent'].max():.4f}]\")\n",
    "print(f\"  Latent has NaN: {torch.isnan(test_batch['latent']).any()}\")\n",
    "print(f\"  Speaker latent shape: {test_batch['speaker_latent'].shape}\")\n",
    "print(f\"  Speaker latent range: [{test_batch['speaker_latent'].min():.4f}, {test_batch['speaker_latent'].max():.4f}]\")\n",
    "print(f\"  Speaker latent has NaN: {torch.isnan(test_batch['speaker_latent']).any()}\")\n",
    "print(f\"  Texts: {test_batch['text']}\")\n",
    "\n",
    "# Try forward pass\n",
    "try:\n",
    "    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        loss = training_step(model, test_batch, DEVICE)\n",
    "    print(f\"\\nTest loss: {loss.item():.4f}\")\n",
    "    print(\"✓ Forward pass successful!\")\n",
    "    \n",
    "    # Try backward pass\n",
    "    loss.backward()\n",
    "    print(\"✓ Backward pass successful!\")\n",
    "    \n",
    "    # Check gradients\n",
    "    grad_norms = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            grad_norms.append((name, grad_norm))\n",
    "            if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
    "                print(f\"  ⚠️  NaN/Inf gradient in {name}\")\n",
    "    \n",
    "    if grad_norms:\n",
    "        # Sort by gradient magnitude\n",
    "        grad_norms.sort(key=lambda x: x[1], reverse=True)\n",
    "        print(f\"\\nTop 5 gradient magnitudes:\")\n",
    "        for name, norm in grad_norms[:5]:\n",
    "            print(f\"  {name}: {norm:.4f}\")\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error during test step: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with validation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"epoch\": [], \"lr\": []}\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train one epoch (scheduler is now passed and called inside train_epoch)\n",
    "    train_loss = train_epoch(\n",
    "        model=model,\n",
    "        dataloader=train_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,  # Pass scheduler to training loop\n",
    "        device=DEVICE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "        max_grad_norm=MAX_GRAD_NORM,\n",
    "        scaler=scaler,\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = validate(model, val_dataloader, DEVICE)\n",
    "    \n",
    "    # Get current LR (scheduler is stepped inside train_epoch now)\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    \n",
    "    # Record history\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"epoch\"].append(epoch + 1)\n",
    "    history[\"lr\"].append(current_lr)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} - Train: {train_loss:.4f} - Val: {val_loss:.4f} - LR: {current_lr:.2e}\")\n",
    "    \n",
    "    # Save best checkpoint (based on validation loss)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        save_lora_checkpoint(\n",
    "            model,\n",
    "            os.path.join(OUTPUT_DIR, \"lora_best.pt\"),\n",
    "            config={\n",
    "                \"rank\": LORA_RANK,\n",
    "                \"alpha\": LORA_ALPHA,\n",
    "                \"dropout\": LORA_DROPOUT,\n",
    "                \"target_modules\": TARGET_MODULES,\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "            }\n",
    "        )\n",
    "        print(f\"  -> Saved best checkpoint (val_loss: {best_val_loss:.4f})\")\n",
    "    \n",
    "    # Periodic checkpoint every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        save_lora_checkpoint(\n",
    "            model,\n",
    "            os.path.join(OUTPUT_DIR, f\"lora_epoch_{epoch + 1}.pt\"),\n",
    "            config={\n",
    "                \"rank\": LORA_RANK,\n",
    "                \"alpha\": LORA_ALPHA,\n",
    "                \"dropout\": LORA_DROPOUT,\n",
    "                \"target_modules\": TARGET_MODULES,\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Save final checkpoint\n",
    "save_lora_checkpoint(\n",
    "    model,\n",
    "    os.path.join(OUTPUT_DIR, \"lora_final.pt\"),\n",
    "    config={\n",
    "        \"rank\": LORA_RANK,\n",
    "        \"alpha\": LORA_ALPHA,\n",
    "        \"dropout\": LORA_DROPOUT,\n",
    "        \"target_modules\": TARGET_MODULES,\n",
    "        \"epoch\": NUM_EPOCHS,\n",
    "        \"train_loss\": history[\"train_loss\"][-1],\n",
    "        \"val_loss\": history[\"val_loss\"][-1],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training complete!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Checkpoints saved to: {OUTPUT_DIR}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Training and validation loss\n",
    "axes[0].plot(history[\"epoch\"], history[\"train_loss\"], 'b-', linewidth=2, label='Train')\n",
    "axes[0].plot(history[\"epoch\"], history[\"val_loss\"], 'r--', linewidth=2, label='Val')\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Training vs Validation Loss\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Training loss only (zoomed)\n",
    "axes[1].plot(history[\"epoch\"], history[\"train_loss\"], 'b-', linewidth=2)\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Loss\")\n",
    "axes[1].set_title(\"Training Loss\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[2].plot(history[\"epoch\"], history[\"lr\"], 'g-', linewidth=2)\n",
    "axes[2].set_xlabel(\"Epoch\")\n",
    "axes[2].set_ylabel(\"Learning Rate\")\n",
    "axes[2].set_title(\"Learning Rate Schedule\")\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"training_curves.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Final train loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"  Final val loss: {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"  Best val loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation & Inference\n",
    "\n",
    "Generate samples with your fine-tuned model and compare to the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Test prompts - rap style!\n",
    "TEST_PROMPTS = [\n",
    "    \"[S1] Yeah, I'm spitting fire on the mic tonight, gonna show them what I got, rising to the top!\",\n",
    "    \"[S1] The rhythm flows through me like water, every beat hits harder, I'm a natural born starter!\",\n",
    "    \"[S1] Check it out, I'm the one they've been waiting for, coming through the door, ready to explore!\",\n",
    "    \"[S1] Money on my mind, grind never stops, from the bottom to the top, watch me drop!\",\n",
    "    \"[S1] Real recognize real, that's the deal, keep it trill, got the skill to make you feel!\",\n",
    "]\n",
    "\n",
    "# Use a random training file as speaker reference\n",
    "SPEAKER_AUDIO_PATH = random.choice(audio_files) if audio_files else None\n",
    "\n",
    "print(f\"Using speaker reference: {SPEAKER_AUDIO_PATH}\")\n",
    "print(f\"\\nWill generate {len(TEST_PROMPTS)} samples...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples with fine-tuned model\n",
    "@torch.inference_mode()\n",
    "def generate_audio(model, text, speaker_audio_path=None, seed=0):\n",
    "    \"\"\"Generate audio using the (fine-tuned) model.\"\"\"\n",
    "    \n",
    "    # Load speaker audio\n",
    "    if speaker_audio_path:\n",
    "        speaker_audio = load_audio(speaker_audio_path)\n",
    "    else:\n",
    "        speaker_audio = None\n",
    "    \n",
    "    # Create sample function\n",
    "    sample_fn = partial(\n",
    "        sample_euler_cfg_independent_guidances,\n",
    "        num_steps=40,\n",
    "        cfg_scale_text=3.0,\n",
    "        cfg_scale_speaker=8.0,\n",
    "        cfg_min_t=0.5,\n",
    "        cfg_max_t=1.0,\n",
    "        truncation_factor=0.8,\n",
    "        rescale_k=None,\n",
    "        rescale_sigma=None,\n",
    "        speaker_kv_scale=None,\n",
    "        speaker_kv_max_layers=None,\n",
    "        speaker_kv_min_t=None,\n",
    "        sequence_length=640,\n",
    "    )\n",
    "    \n",
    "    # Generate\n",
    "    audio_out, normalized_text = sample_pipeline(\n",
    "        model=model,\n",
    "        fish_ae=fish_ae,\n",
    "        pca_state=pca_state,\n",
    "        sample_fn=sample_fn,\n",
    "        text_prompt=text,\n",
    "        speaker_audio=speaker_audio,\n",
    "        rng_seed=seed,\n",
    "    )\n",
    "    \n",
    "    return audio_out[0].cpu(), normalized_text\n",
    "\n",
    "# Generate and play samples\n",
    "print(\"Generating samples with fine-tuned model...\\n\")\n",
    "\n",
    "for i, prompt in enumerate(TEST_PROMPTS):\n",
    "    print(f\"Prompt {i + 1}: {prompt}\")\n",
    "    \n",
    "    audio, _ = generate_audio(\n",
    "        model,\n",
    "        prompt,\n",
    "        speaker_audio_path=str(SPEAKER_AUDIO_PATH) if SPEAKER_AUDIO_PATH else None,\n",
    "        seed=i,\n",
    "    )\n",
    "    \n",
    "    # Save audio\n",
    "    output_path = os.path.join(OUTPUT_DIR, f\"sample_{i + 1}.wav\")\n",
    "    torchaudio.save(output_path, audio.unsqueeze(0), 44100)\n",
    "    print(f\"Saved to: {output_path}\")\n",
    "    \n",
    "    # Play audio\n",
    "    display(Audio(audio.numpy(), rate=44100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Checkpoint for Later Use\n",
    "\n",
    "Use this section to load a saved LoRA checkpoint onto a fresh model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load a saved LoRA checkpoint\n",
    "# Uncomment and run this cell to load a checkpoint\n",
    "\n",
    "# CHECKPOINT_PATH = os.path.join(OUTPUT_DIR, \"lora_best.pt\")\n",
    "# \n",
    "# # Load fresh base model\n",
    "# model_fresh = load_model_from_hf(\n",
    "#     device=DEVICE,\n",
    "#     dtype=DTYPE,\n",
    "#     compile=False,\n",
    "#     delete_blockwise_modules=True,\n",
    "# )\n",
    "# \n",
    "# # Load checkpoint to get config\n",
    "# checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "# config = checkpoint[\"config\"]\n",
    "# \n",
    "# # Apply LoRA with saved config\n",
    "# model_fresh, _ = apply_lora_to_model(\n",
    "#     model_fresh,\n",
    "#     rank=config[\"rank\"],\n",
    "#     alpha=config[\"alpha\"],\n",
    "#     dropout=0.0,  # No dropout for inference\n",
    "#     target_modules=config[\"target_modules\"],\n",
    "# )\n",
    "# \n",
    "# # Load LoRA weights\n",
    "# load_lora_checkpoint(model_fresh, CHECKPOINT_PATH, device=DEVICE)\n",
    "# model_fresh.eval()\n",
    "# \n",
    "# print(f\"Loaded checkpoint from epoch {config['epoch']} (loss: {config['loss']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Tips & Notes for Rap Training\n",
    "\n",
    "### What We Configured for 225 Samples\n",
    "- **LoRA rank 32**: Higher rank captures more style nuance with larger dataset\n",
    "- **Lower dropout (0.05)**: Less regularization needed with more data\n",
    "- **Higher learning rate (1e-4)**: Can train faster with more data\n",
    "- **Fewer epochs (10)**: More data means fewer passes needed\n",
    "- **Whisper medium**: Better accuracy for rap lyrics than base model\n",
    "\n",
    "### Training Expectations\n",
    "With 225 rap acapellas:\n",
    "- **Training time**: ~2-4 hours on T4, ~1-2 hours on A100\n",
    "- **Expected final loss**: ~0.05-0.15 (lower is better)\n",
    "- **Checkpoint size**: ~80-100MB\n",
    "\n",
    "### If Results Sound Off\n",
    "1. **Too monotone**: Increase `cfg_scale_text` to 4-5 during inference\n",
    "2. **Wrong rhythm**: The model learned general rap style, not specific flows\n",
    "3. **Voice doesn't match**: Try different speaker reference audio\n",
    "4. **Gibberish output**: Check if transcriptions were accurate\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "**Out of Memory (OOM)**:\n",
    "- Reduce `MAX_LATENT_LENGTH` to 320 (15 seconds max)\n",
    "- Use A100 GPU on Colab instead of T4\n",
    "- Reduce `LORA_RANK` to 16\n",
    "\n",
    "**Loss not decreasing**:\n",
    "- Check transcriptions are accurate (rap lyrics are hard!)\n",
    "- Try `WHISPER_MODEL = \"large-v3\"` for better transcription\n",
    "\n",
    "**Validation loss increasing (overfitting)**:\n",
    "- Increase `LORA_DROPOUT` to 0.1\n",
    "- Reduce `NUM_EPOCHS`\n",
    "- Reduce `LORA_RANK` to 16\n",
    "\n",
    "### Voice Cloning Still Works!\n",
    "The speaker path (wk_speaker, wv_speaker) was kept frozen, so you can:\n",
    "- Use ANY speaker reference audio at inference time\n",
    "- The rap style transfers to any voice\n",
    "- Original voice cloning quality is preserved"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
