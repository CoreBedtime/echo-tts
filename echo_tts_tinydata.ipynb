{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Echo-TTS Rap Style Fine-Tuning\n\nThis notebook implements LoRA-based fine-tuning for Echo-TTS to adapt the model to **rapping style** while preserving voice cloning capabilities.\n\n**Dataset:**\n- 225 rap acapellas (unprocessed)\n- Requires preprocessing: segmentation, transcription\n\n**Requirements:**\n- GPU with 16GB+ VRAM (T4/A100 on Colab)\n\n**What you'll get:**\n- Fine-tuned model that generates rap-style speech\n- LoRA checkpoint (~50-100MB) that can be loaded on top of base model\n- Preserved voice cloning - can still use any speaker reference"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone the echo-tts repository\n!git clone https://github.com/CoreBedtime/echo-tts.git\n%cd echo-tts\n\n# Install dependencies\n!pip install -q torch torchaudio safetensors huggingface-hub einops\n!pip install -q openai-whisper  # For automatic transcription\n!pip install -q torchcodec  # For audio decoding"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for saving checkpoints (Colab)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# PATHS - Update these!\n# =============================================================================\n\n# Directory containing your training audio files\nAUDIO_DIR = \"./training_audio/\"  # Put your rap acapellas here\n\n# Where to save checkpoints\nOUTPUT_DIR = \"./checkpoints/\"\nif IN_COLAB:\n    OUTPUT_DIR = \"/content/drive/MyDrive/echo_tts_rap_checkpoints/\"\n\n# =============================================================================\n# LoRA CONFIGURATION (optimized for 225 samples)\n# =============================================================================\n\nLORA_RANK = 32          # Higher rank for larger dataset (more expressiveness)\nLORA_ALPHA = 32.0       # Scaling factor, typically equal to rank\nLORA_DROPOUT = 0.05     # Lower dropout - more data means less regularization needed\n\n# Which modules to train (default preserves voice cloning path)\n# Speaker path (wk_speaker, wv_speaker) is NOT trained to preserve cloning\nTARGET_MODULES = [\n    # Main decoder attention (style)\n    \"blocks.*.attention.wq\",\n    \"blocks.*.attention.wk\",\n    \"blocks.*.attention.wv\",\n    \"blocks.*.attention.wo\",\n    # Text cross-attention (text-to-audio mapping)\n    \"blocks.*.attention.wk_text\",\n    \"blocks.*.attention.wv_text\",\n    # MLP layers (feature transformation)\n    \"blocks.*.mlp.w1\",\n    \"blocks.*.mlp.w2\",\n    \"blocks.*.mlp.w3\",\n]\n\n# =============================================================================\n# TRAINING CONFIGURATION (optimized for 225 samples)\n# =============================================================================\n\nLEARNING_RATE = 1e-4     # Slightly higher LR for larger dataset\nNUM_EPOCHS = 10          # Fewer epochs needed with more data\nBATCH_SIZE = 1           # Batch size (1 for memory efficiency)\nGRADIENT_ACCUMULATION = 8  # Larger effective batch for stability\nMAX_GRAD_NORM = 1.0      # Gradient clipping\nWARMUP_STEPS = 100       # More warmup steps for larger dataset\n\n# Audio settings\nMAX_LATENT_LENGTH = 640  # Max ~30 seconds (reduce to 320 if OOM)\nSEGMENT_DURATION = 25.0  # Split long audio into ~25 second chunks\nMIN_SEGMENT_DURATION = 5.0  # Minimum segment length to keep\n\n# Whisper model for transcription (use larger model for rap lyrics accuracy)\nWHISPER_MODEL = \"medium\"  # Options: tiny, base, small, medium, large-v3\n\n# Device\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nDTYPE = torch.bfloat16   # Use bfloat16 for training\n\n# Validation split\nVAL_SPLIT = 0.05  # 5% for validation (~11 samples)\n\n# Create output directory\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(AUDIO_DIR, exist_ok=True)\n\nprint(f\"Audio directory: {AUDIO_DIR}\")\nprint(f\"Output directory: {OUTPUT_DIR}\")\nprint(f\"Device: {DEVICE}\")\nprint(f\"\\nTraining config for 225 samples:\")\nprint(f\"  LoRA rank: {LORA_RANK}\")\nprint(f\"  Learning rate: {LEARNING_RATE}\")\nprint(f\"  Epochs: {NUM_EPOCHS}\")\nprint(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import (\n",
    "    load_model_from_hf,\n",
    "    load_fish_ae_from_hf,\n",
    "    load_pca_state_from_hf,\n",
    "    ae_decode,\n",
    "    get_speaker_latent_and_mask,\n",
    "    get_text_input_ids_and_mask,\n",
    "    sample_euler_cfg_independent_guidances,\n",
    "    sample_pipeline,\n",
    "    load_audio,\n",
    ")\n",
    "from functools import partial\n",
    "\n",
    "print(\"Loading EchoDiT model...\")\n",
    "model = load_model_from_hf(\n",
    "    device=DEVICE,\n",
    "    dtype=DTYPE,\n",
    "    compile=False,  # Don't compile for training\n",
    "    delete_blockwise_modules=True,  # Save memory\n",
    ")\n",
    "print(f\"Model loaded: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M parameters\")\n",
    "\n",
    "print(\"\\nLoading Fish-S1-DAC autoencoder...\")\n",
    "fish_ae = load_fish_ae_from_hf(\n",
    "    device=DEVICE,\n",
    "    dtype=torch.float32,  # AE needs float32 for quality\n",
    ")\n",
    "print(\"Autoencoder loaded\")\n",
    "\n",
    "print(\"\\nLoading PCA state...\")\n",
    "pca_state = load_pca_state_from_hf(device=DEVICE)\n",
    "print(\"PCA state loaded\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"All models loaded successfully!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Data Preparation\n\nWith 225 rap acapellas, we need to:\n1. List and validate all audio files\n2. Segment long tracks into training-sized chunks (~25 seconds)\n3. Transcribe using Whisper (medium model for better rap lyrics)\n4. Create train/validation split\n5. Pre-encode all audio to latents (cached for fast training)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# List and analyze all audio files\nfrom pathlib import Path\nimport random\n\nAUDIO_EXTENSIONS = (\".mp3\", \".wav\", \".flac\", \".ogg\", \".m4a\")\n\naudio_dir = Path(AUDIO_DIR)\naudio_files = []\nfor ext in AUDIO_EXTENSIONS:\n    audio_files.extend(audio_dir.glob(f\"*{ext}\"))\n    audio_files.extend(audio_dir.glob(f\"*{ext.upper()}\"))\n    # Also check subdirectories\n    audio_files.extend(audio_dir.glob(f\"**/*{ext}\"))\n    audio_files.extend(audio_dir.glob(f\"**/*{ext.upper()}\"))\n\n# Remove duplicates and sort\naudio_files = sorted(set(audio_files))\n\nprint(f\"Found {len(audio_files)} audio files\")\nprint(\"=\"*50)\n\n# Analyze durations\ndurations = []\nvalid_files = []\nfor f in audio_files:\n    try:\n        info = torchaudio.info(str(f))\n        duration = info.num_frames / info.sample_rate\n        durations.append(duration)\n        valid_files.append(f)\n    except Exception as e:\n        print(f\"  Skipping {f.name}: {e}\")\n\naudio_files = valid_files\ntotal_duration = sum(durations)\n\nprint(f\"\\nDataset Statistics:\")\nprint(f\"  Total files: {len(audio_files)}\")\nprint(f\"  Total duration: {total_duration:.1f}s ({total_duration/60:.1f} min, {total_duration/3600:.2f} hours)\")\nprint(f\"  Average duration: {sum(durations)/len(durations):.1f}s\")\nprint(f\"  Shortest: {min(durations):.1f}s\")\nprint(f\"  Longest: {max(durations):.1f}s\")\n\n# Show duration distribution\nprint(f\"\\nDuration distribution:\")\nfor bucket in [(0, 30), (30, 60), (60, 120), (120, 180), (180, 300), (300, float('inf'))]:\n    count = sum(1 for d in durations if bucket[0] <= d < bucket[1])\n    if count > 0:\n        label = f\"{bucket[0]}-{bucket[1]}s\" if bucket[1] != float('inf') else f\">{bucket[0]}s\"\n        print(f\"  {label}: {count} files\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Transcribe all audio files using Whisper\n# This will take a while for 225 files - save progress as we go\n\nfrom train_utils import transcribe_audio_files\nimport json\n\nTRANSCRIPTION_CACHE = os.path.join(OUTPUT_DIR, \"transcriptions.json\")\n\n# Check if we have cached transcriptions\nif os.path.exists(TRANSCRIPTION_CACHE):\n    print(f\"Loading cached transcriptions from {TRANSCRIPTION_CACHE}\")\n    with open(TRANSCRIPTION_CACHE, \"r\") as f:\n        transcriptions = json.load(f)\n    print(f\"Loaded {len(transcriptions)} cached transcriptions\")\n    \n    # Find files that still need transcription\n    cached_paths = set(transcriptions.keys())\n    files_to_transcribe = [f for f in audio_files if str(f) not in cached_paths]\n    print(f\"Files still needing transcription: {len(files_to_transcribe)}\")\nelse:\n    transcriptions = {}\n    files_to_transcribe = audio_files\n\nif len(files_to_transcribe) > 0:\n    print(f\"\\nTranscribing {len(files_to_transcribe)} files with Whisper ({WHISPER_MODEL})...\")\n    print(\"This may take 30-60 minutes for 225 files...\")\n    print(\"Progress is saved - you can interrupt and resume.\\n\")\n    \n    # Transcribe in batches and save progress\n    BATCH_SIZE_TRANSCRIBE = 10\n    \n    for batch_start in range(0, len(files_to_transcribe), BATCH_SIZE_TRANSCRIBE):\n        batch_files = files_to_transcribe[batch_start:batch_start + BATCH_SIZE_TRANSCRIBE]\n        \n        batch_transcriptions = transcribe_audio_files(\n            audio_paths=[str(f) for f in batch_files],\n            model_name=WHISPER_MODEL,\n            language=\"en\",\n        )\n        \n        transcriptions.update(batch_transcriptions)\n        \n        # Save progress\n        with open(TRANSCRIPTION_CACHE, \"w\") as f:\n            json.dump(transcriptions, f, indent=2)\n        \n        print(f\"Progress: {min(batch_start + BATCH_SIZE_TRANSCRIBE, len(files_to_transcribe))}/{len(files_to_transcribe)} transcribed\")\n\nprint(f\"\\n{'='*50}\")\nprint(f\"Transcription complete: {len(transcriptions)} files\")\nprint(f\"Saved to: {TRANSCRIPTION_CACHE}\")\n\n# Show a few samples\nprint(f\"\\nSample transcriptions:\")\nfor i, (path, text) in enumerate(list(transcriptions.items())[:3]):\n    filename = Path(path).name\n    print(f\"\\n{filename}:\")\n    print(f\"  {text[:150]}{'...' if len(text) > 150 else ''}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Edit transcriptions manually if Whisper made mistakes\n",
    "# Uncomment and modify as needed:\n",
    "\n",
    "# transcriptions[\"/path/to/file.mp3\"] = \"[S1] Your corrected transcription here.\"\n",
    "\n",
    "# Tips for transcriptions:\n",
    "# - Start with [S1] for single speaker\n",
    "# - Use commas for pauses\n",
    "# - Exclamation marks increase expressiveness\n",
    "# - Keep punctuation natural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create training dataset with train/val split\nfrom train_utils import (\n    TrainingSample,\n    EchoTTSDataset,\n    collate_fn,\n    segment_audio,\n    load_audio_tensor,\n)\nfrom torch.utils.data import DataLoader\nimport random\n\n# Create training samples\nall_samples = []\nfor path, text in transcriptions.items():\n    if text and len(text.strip()) > 10:  # Filter out empty/very short transcriptions\n        all_samples.append(TrainingSample(\n            audio_path=path,\n            text=text,\n            speaker_audio_path=None,  # Use same audio as speaker reference\n        ))\n\nprint(f\"Created {len(all_samples)} training samples (filtered {len(transcriptions) - len(all_samples)} empty)\")\n\n# Shuffle and split into train/val\nrandom.seed(42)\nrandom.shuffle(all_samples)\n\nval_size = max(1, int(len(all_samples) * VAL_SPLIT))\ntrain_samples = all_samples[val_size:]\nval_samples = all_samples[:val_size]\n\nprint(f\"Train samples: {len(train_samples)}\")\nprint(f\"Validation samples: {len(val_samples)}\")\n\n# Create datasets\nprint(\"\\nCreating training dataset and encoding audio to latents...\")\nprint(\"This will take a few minutes for 225 files...\\n\")\n\ntrain_dataset = EchoTTSDataset(\n    samples=train_samples,\n    fish_ae=fish_ae,\n    pca_state=pca_state,\n    device=DEVICE,\n    max_latent_length=MAX_LATENT_LENGTH,\n    cache_latents=True,\n)\n\nprint(\"\\nCreating validation dataset...\")\nval_dataset = EchoTTSDataset(\n    samples=val_samples,\n    fish_ae=fish_ae,\n    pca_state=pca_state,\n    device=DEVICE,\n    max_latent_length=MAX_LATENT_LENGTH,\n    cache_latents=True,\n)\n\n# Create dataloaders\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=0,\n)\n\nval_dataloader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    collate_fn=collate_fn,\n    num_workers=0,\n)\n\nprint(f\"\\n{'='*50}\")\nprint(f\"Dataset ready!\")\nprint(f\"  Train: {len(train_dataset)} samples, {len(train_dataloader)} batches/epoch\")\nprint(f\"  Val: {len(val_dataset)} samples, {len(val_dataloader)} batches\")\nprint(f\"{'='*50}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply LoRA to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lora import (\n",
    "    apply_lora_to_model,\n",
    "    count_parameters,\n",
    "    get_lora_params,\n",
    "    save_lora_checkpoint,\n",
    "    load_lora_checkpoint,\n",
    ")\n",
    "\n",
    "# Apply LoRA adapters to the model\n",
    "print(\"Applying LoRA adapters...\")\n",
    "print(f\"  Rank: {LORA_RANK}\")\n",
    "print(f\"  Alpha: {LORA_ALPHA}\")\n",
    "print(f\"  Dropout: {LORA_DROPOUT}\")\n",
    "print(f\"  Target modules: {len(TARGET_MODULES)} patterns\")\n",
    "\n",
    "model, lora_modules = apply_lora_to_model(\n",
    "    model,\n",
    "    rank=LORA_RANK,\n",
    "    alpha=LORA_ALPHA,\n",
    "    dropout=LORA_DROPOUT,\n",
    "    target_modules=TARGET_MODULES,\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "print(f\"\\nParameter counts:\")\n",
    "print(f\"  Total: {total_params / 1e6:.1f}M\")\n",
    "print(f\"  Trainable (LoRA): {trainable_params / 1e6:.2f}M ({100 * trainable_params / total_params:.2f}%)\")\n",
    "print(f\"  LoRA modules applied: {len(lora_modules)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from train_utils import train_epoch, get_cosine_schedule_with_warmup, training_step\n\n# Setup optimizer (only LoRA params)\nlora_params = get_lora_params(model)\noptimizer = torch.optim.AdamW(\n    lora_params,\n    lr=LEARNING_RATE,\n    weight_decay=0.01,\n    betas=(0.9, 0.999),\n)\n\n# Learning rate scheduler\nnum_training_steps = len(train_dataloader) * NUM_EPOCHS // GRADIENT_ACCUMULATION\nscheduler = get_cosine_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=min(WARMUP_STEPS, num_training_steps // 10),\n    num_training_steps=num_training_steps,\n)\n\n# Mixed precision scaler\nscaler = torch.cuda.amp.GradScaler()\n\nprint(\"Training setup:\")\nprint(f\"  Epochs: {NUM_EPOCHS}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION}\")\nprint(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\nprint(f\"  Learning rate: {LEARNING_RATE}\")\nprint(f\"  Training steps: {num_training_steps}\")\nprint(f\"  Warmup steps: {min(WARMUP_STEPS, num_training_steps // 10)}\")\nprint(f\"  Train batches/epoch: {len(train_dataloader)}\")\nprint(f\"  Val batches: {len(val_dataloader)}\")\n\n# Validation function\n@torch.no_grad()\ndef validate(model, val_dataloader, device):\n    model.eval()\n    total_loss = 0.0\n    num_batches = 0\n    \n    for batch in val_dataloader:\n        with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n            loss = training_step(model, batch, device)\n        total_loss += loss.item()\n        num_batches += 1\n    \n    model.train()\n    return total_loss / max(num_batches, 1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training loop with validation\nprint(\"\\n\" + \"=\"*50)\nprint(\"Starting training...\")\nprint(\"=\"*50 + \"\\n\")\n\nhistory = {\"train_loss\": [], \"val_loss\": [], \"epoch\": [], \"lr\": []}\nbest_val_loss = float(\"inf\")\n\nfor epoch in range(NUM_EPOCHS):\n    # Train one epoch\n    train_loss = train_epoch(\n        model=model,\n        dataloader=train_dataloader,\n        optimizer=optimizer,\n        device=DEVICE,\n        gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n        max_grad_norm=MAX_GRAD_NORM,\n        scaler=scaler,\n    )\n    \n    # Validate\n    val_loss = validate(model, val_dataloader, DEVICE)\n    \n    # Step scheduler\n    scheduler.step()\n    current_lr = scheduler.get_last_lr()[0]\n    \n    # Record history\n    history[\"train_loss\"].append(train_loss)\n    history[\"val_loss\"].append(val_loss)\n    history[\"epoch\"].append(epoch + 1)\n    history[\"lr\"].append(current_lr)\n    \n    # Print progress\n    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} - Train: {train_loss:.4f} - Val: {val_loss:.4f} - LR: {current_lr:.2e}\")\n    \n    # Save best checkpoint (based on validation loss)\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        save_lora_checkpoint(\n            model,\n            os.path.join(OUTPUT_DIR, \"lora_best.pt\"),\n            config={\n                \"rank\": LORA_RANK,\n                \"alpha\": LORA_ALPHA,\n                \"dropout\": LORA_DROPOUT,\n                \"target_modules\": TARGET_MODULES,\n                \"epoch\": epoch + 1,\n                \"train_loss\": train_loss,\n                \"val_loss\": val_loss,\n            }\n        )\n        print(f\"  -> Saved best checkpoint (val_loss: {best_val_loss:.4f})\")\n    \n    # Periodic checkpoint every 2 epochs\n    if (epoch + 1) % 2 == 0:\n        save_lora_checkpoint(\n            model,\n            os.path.join(OUTPUT_DIR, f\"lora_epoch_{epoch + 1}.pt\"),\n            config={\n                \"rank\": LORA_RANK,\n                \"alpha\": LORA_ALPHA,\n                \"dropout\": LORA_DROPOUT,\n                \"target_modules\": TARGET_MODULES,\n                \"epoch\": epoch + 1,\n                \"train_loss\": train_loss,\n                \"val_loss\": val_loss,\n            }\n        )\n\n# Save final checkpoint\nsave_lora_checkpoint(\n    model,\n    os.path.join(OUTPUT_DIR, \"lora_final.pt\"),\n    config={\n        \"rank\": LORA_RANK,\n        \"alpha\": LORA_ALPHA,\n        \"dropout\": LORA_DROPOUT,\n        \"target_modules\": TARGET_MODULES,\n        \"epoch\": NUM_EPOCHS,\n        \"train_loss\": history[\"train_loss\"][-1],\n        \"val_loss\": history[\"val_loss\"][-1],\n    }\n)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Training complete!\")\nprint(f\"Best validation loss: {best_val_loss:.4f}\")\nprint(f\"Checkpoints saved to: {OUTPUT_DIR}\")\nprint(\"=\"*50)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot training curves\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Training and validation loss\naxes[0].plot(history[\"epoch\"], history[\"train_loss\"], 'b-', linewidth=2, label='Train')\naxes[0].plot(history[\"epoch\"], history[\"val_loss\"], 'r--', linewidth=2, label='Val')\naxes[0].set_xlabel(\"Epoch\")\naxes[0].set_ylabel(\"Loss\")\naxes[0].set_title(\"Training vs Validation Loss\")\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Training loss only (zoomed)\naxes[1].plot(history[\"epoch\"], history[\"train_loss\"], 'b-', linewidth=2)\naxes[1].set_xlabel(\"Epoch\")\naxes[1].set_ylabel(\"Loss\")\naxes[1].set_title(\"Training Loss\")\naxes[1].grid(True, alpha=0.3)\n\n# Learning rate\naxes[2].plot(history[\"epoch\"], history[\"lr\"], 'g-', linewidth=2)\naxes[2].set_xlabel(\"Epoch\")\naxes[2].set_ylabel(\"Learning Rate\")\naxes[2].set_title(\"Learning Rate Schedule\")\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(os.path.join(OUTPUT_DIR, \"training_curves.png\"), dpi=150)\nplt.show()\n\n# Print summary\nprint(f\"\\nTraining Summary:\")\nprint(f\"  Final train loss: {history['train_loss'][-1]:.4f}\")\nprint(f\"  Final val loss: {history['val_loss'][-1]:.4f}\")\nprint(f\"  Best val loss: {best_val_loss:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation & Inference\n",
    "\n",
    "Generate samples with your fine-tuned model and compare to the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set model to eval mode\nmodel.eval()\n\n# Test prompts - rap style!\nTEST_PROMPTS = [\n    \"[S1] Yeah, I'm spitting fire on the mic tonight, gonna show them what I got, rising to the top!\",\n    \"[S1] The rhythm flows through me like water, every beat hits harder, I'm a natural born starter!\",\n    \"[S1] Check it out, I'm the one they've been waiting for, coming through the door, ready to explore!\",\n    \"[S1] Money on my mind, grind never stops, from the bottom to the top, watch me drop!\",\n    \"[S1] Real recognize real, that's the deal, keep it trill, got the skill to make you feel!\",\n]\n\n# Use a random training file as speaker reference\nSPEAKER_AUDIO_PATH = random.choice(audio_files) if audio_files else None\n\nprint(f\"Using speaker reference: {SPEAKER_AUDIO_PATH}\")\nprint(f\"\\nWill generate {len(TEST_PROMPTS)} samples...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples with fine-tuned model\n",
    "@torch.inference_mode()\n",
    "def generate_audio(model, text, speaker_audio_path=None, seed=0):\n",
    "    \"\"\"Generate audio using the (fine-tuned) model.\"\"\"\n",
    "    \n",
    "    # Load speaker audio\n",
    "    if speaker_audio_path:\n",
    "        speaker_audio = load_audio(speaker_audio_path)\n",
    "    else:\n",
    "        speaker_audio = None\n",
    "    \n",
    "    # Create sample function\n",
    "    sample_fn = partial(\n",
    "        sample_euler_cfg_independent_guidances,\n",
    "        num_steps=40,\n",
    "        cfg_scale_text=3.0,\n",
    "        cfg_scale_speaker=8.0,\n",
    "        cfg_min_t=0.5,\n",
    "        cfg_max_t=1.0,\n",
    "        truncation_factor=0.8,\n",
    "        rescale_k=None,\n",
    "        rescale_sigma=None,\n",
    "        speaker_kv_scale=None,\n",
    "        speaker_kv_max_layers=None,\n",
    "        speaker_kv_min_t=None,\n",
    "        sequence_length=640,\n",
    "    )\n",
    "    \n",
    "    # Generate\n",
    "    audio_out, normalized_text = sample_pipeline(\n",
    "        model=model,\n",
    "        fish_ae=fish_ae,\n",
    "        pca_state=pca_state,\n",
    "        sample_fn=sample_fn,\n",
    "        text_prompt=text,\n",
    "        speaker_audio=speaker_audio,\n",
    "        rng_seed=seed,\n",
    "    )\n",
    "    \n",
    "    return audio_out[0].cpu(), normalized_text\n",
    "\n",
    "# Generate and play samples\n",
    "print(\"Generating samples with fine-tuned model...\\n\")\n",
    "\n",
    "for i, prompt in enumerate(TEST_PROMPTS):\n",
    "    print(f\"Prompt {i + 1}: {prompt}\")\n",
    "    \n",
    "    audio, _ = generate_audio(\n",
    "        model,\n",
    "        prompt,\n",
    "        speaker_audio_path=str(SPEAKER_AUDIO_PATH) if SPEAKER_AUDIO_PATH else None,\n",
    "        seed=i,\n",
    "    )\n",
    "    \n",
    "    # Save audio\n",
    "    output_path = os.path.join(OUTPUT_DIR, f\"sample_{i + 1}.wav\")\n",
    "    torchaudio.save(output_path, audio.unsqueeze(0), 44100)\n",
    "    print(f\"Saved to: {output_path}\")\n",
    "    \n",
    "    # Play audio\n",
    "    display(Audio(audio.numpy(), rate=44100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Checkpoint for Later Use\n",
    "\n",
    "Use this section to load a saved LoRA checkpoint onto a fresh model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load a saved LoRA checkpoint\n",
    "# Uncomment and run this cell to load a checkpoint\n",
    "\n",
    "# CHECKPOINT_PATH = os.path.join(OUTPUT_DIR, \"lora_best.pt\")\n",
    "# \n",
    "# # Load fresh base model\n",
    "# model_fresh = load_model_from_hf(\n",
    "#     device=DEVICE,\n",
    "#     dtype=DTYPE,\n",
    "#     compile=False,\n",
    "#     delete_blockwise_modules=True,\n",
    "# )\n",
    "# \n",
    "# # Load checkpoint to get config\n",
    "# checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "# config = checkpoint[\"config\"]\n",
    "# \n",
    "# # Apply LoRA with saved config\n",
    "# model_fresh, _ = apply_lora_to_model(\n",
    "#     model_fresh,\n",
    "#     rank=config[\"rank\"],\n",
    "#     alpha=config[\"alpha\"],\n",
    "#     dropout=0.0,  # No dropout for inference\n",
    "#     target_modules=config[\"target_modules\"],\n",
    "# )\n",
    "# \n",
    "# # Load LoRA weights\n",
    "# load_lora_checkpoint(model_fresh, CHECKPOINT_PATH, device=DEVICE)\n",
    "# model_fresh.eval()\n",
    "# \n",
    "# print(f\"Loaded checkpoint from epoch {config['epoch']} (loss: {config['loss']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Tips & Notes for Rap Training\n\n### What We Configured for 225 Samples\n- **LoRA rank 32**: Higher rank captures more style nuance with larger dataset\n- **Lower dropout (0.05)**: Less regularization needed with more data\n- **Higher learning rate (1e-4)**: Can train faster with more data\n- **Fewer epochs (10)**: More data means fewer passes needed\n- **Whisper medium**: Better accuracy for rap lyrics than base model\n\n### Training Expectations\nWith 225 rap acapellas:\n- **Training time**: ~2-4 hours on T4, ~1-2 hours on A100\n- **Expected final loss**: ~0.05-0.15 (lower is better)\n- **Checkpoint size**: ~80-100MB\n\n### If Results Sound Off\n1. **Too monotone**: Increase `cfg_scale_text` to 4-5 during inference\n2. **Wrong rhythm**: The model learned general rap style, not specific flows\n3. **Voice doesn't match**: Try different speaker reference audio\n4. **Gibberish output**: Check if transcriptions were accurate\n\n### Common Issues\n\n**Out of Memory (OOM)**:\n- Reduce `MAX_LATENT_LENGTH` to 320 (15 seconds max)\n- Use A100 GPU on Colab instead of T4\n- Reduce `LORA_RANK` to 16\n\n**Loss not decreasing**:\n- Check transcriptions are accurate (rap lyrics are hard!)\n- Try `WHISPER_MODEL = \"large-v3\"` for better transcription\n\n**Validation loss increasing (overfitting)**:\n- Increase `LORA_DROPOUT` to 0.1\n- Reduce `NUM_EPOCHS`\n- Reduce `LORA_RANK` to 16\n\n### Voice Cloning Still Works!\nThe speaker path (wk_speaker, wv_speaker) was kept frozen, so you can:\n- Use ANY speaker reference audio at inference time\n- The rap style transfers to any voice\n- Original voice cloning quality is preserved"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}