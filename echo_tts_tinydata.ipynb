{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Launch the main Gradio app with LoRA support and public sharing\n# This runs the gradio_app.py file which has the full UI and all features\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üöÄ Launching Echo-TTS Gradio Interface...\")\nprint(\"=\"*60)\nprint(\"\\nThe Gradio app will:\")\nprint(\"  - Load the base Echo-TTS model\")\nprint(\"  - Allow loading your LoRA checkpoint via path input\")\nprint(\"  - Generate a public shareable URL\")\nprint(\"\\nTo use your fine-tuned LoRA:\")\nprint(f\"  1. Enter this path in the LoRA section: {OUTPUT_DIR}lora_best.pt\")\nprint(\"  2. Click Generate to test with rap style!\\n\")\n\n# Set environment variable to enable public sharing\nimport os\nos.environ[\"GRADIO_SHARE\"] = \"true\"\n\n# Run the gradio app\n!python gradio_app.py",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Echo-TTS Controllable Rap Style Fine-Tuning\n\nThis notebook implements LoRA-based fine-tuning for Echo-TTS with **controllable rhythm/timing** - enabling you to separate voice identity from delivery style.\n\n**New Feature: Controllable Rhythm/Timing**\n- **Speaker Reference**: Controls WHO it sounds like (voice identity)\n- **Content Reference**: Controls HOW it's delivered (rhythm, pacing, flow)\n- Train the model to follow timing patterns from any audio!\n\n**Dataset:**\n- 173+ rap acapellas with transcriptions\n- Requires: `dataset/transcriptions.json`\n\n**Requirements:**\n- GPU with 16GB+ VRAM (T4/A100 on Colab)\n\n**What you'll get:**\n- Fine-tuned model with controllable rhythm transfer\n- LoRA checkpoint (~50-100MB) loadable on base model\n- Preserved voice cloning - use any speaker reference"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hWorking directory: /content/echo-tts\n"
     ]
    }
   ],
   "source": [
    "# Clone the echo-tts repository\n",
    "!rm -rf /content/echo-tts\n",
    "!git clone https://github.com/CoreBedtime/echo-tts.git 2>/dev/null || echo \"Repo already exists\"\n",
    "\n",
    "# Add to Python path\n",
    "import sys\n",
    "sys.path.insert(0, '/content/echo-tts')\n",
    "\n",
    "# Change working directory\n",
    "import os\n",
    "os.chdir('/content/echo-tts')\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q torch torchaudio safetensors huggingface-hub einops\n",
    "!pip install -q transformers accelerate  # For Parakeet transcription (5-10x faster!)\n",
    "!pip install -q torchcodec  # For audio decoding\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for saving checkpoints (Colab)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu126\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "VRAM: 42.5 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\n\n# =============================================================================\n# PATHS - Update these!\n# =============================================================================\n\n# Directory containing your training audio files\nAUDIO_DIR = \"/content/dataset/\"  # Put your rap acapellas here\n\n# Where to save checkpoints\nOUTPUT_DIR = \"./checkpoints/\" \n\n# =============================================================================\n# LoRA CONFIGURATION (optimized for controllable rhythm training)\n# =============================================================================\n\nLORA_RANK = 32          # Higher rank for larger dataset (more expressiveness)\nLORA_ALPHA = 32.0       # Scaling factor, typically equal to rank\nLORA_DROPOUT = 0.05     # Lower dropout - more data means less regularization needed\n\n# Which modules to train (includes latent path for controllable rhythm!)\n# Speaker path (wk_speaker, wv_speaker) is NOT trained to preserve voice cloning\nTARGET_MODULES = [\n    # Main decoder attention (style)\n    \"blocks.*.attention.wq\",\n    \"blocks.*.attention.wk\",\n    \"blocks.*.attention.wv\",\n    \"blocks.*.attention.wo\",\n    # Text cross-attention (text-to-audio mapping)\n    \"blocks.*.attention.wk_text\",\n    \"blocks.*.attention.wv_text\",\n    # Latent cross-attention (CONTROLLABLE RHYTHM/TIMING!)\n    \"blocks.*.attention.wk_latent\",\n    \"blocks.*.attention.wv_latent\",\n    # MLP layers (feature transformation)\n    \"blocks.*.mlp.w1\",\n    \"blocks.*.mlp.w2\",\n    \"blocks.*.mlp.w3\",\n]\n\n# =============================================================================\n# CONTROLLABLE RHYTHM CONFIGURATION\n# =============================================================================\n\nUSE_CONTENT_CONDITIONING = True  # Enable rhythm/timing training\n# When True, the model learns to follow timing from content latent\n# This enables: same text + different rhythm reference = different delivery!\n\n# =============================================================================\n# TRAINING CONFIGURATION (optimized for 173+ samples)\n# =============================================================================\n\nLEARNING_RATE = 1e-4     # Slightly higher LR for larger dataset\nNUM_EPOCHS = 10          # Fewer epochs needed with more data\nBATCH_SIZE = 1           # Batch size (1 for memory efficiency)\nGRADIENT_ACCUMULATION = 8  # Larger effective batch for stability\nMAX_GRAD_NORM = 1.0      # Gradient clipping\nWARMUP_STEPS = 100       # More warmup steps for larger dataset\n\n# Audio settings\nMAX_LATENT_LENGTH = 640  # Max ~30 seconds (reduce to 320 if OOM)\nSEGMENT_DURATION = 25.0  # Split long audio into ~25 second chunks\nMIN_SEGMENT_DURATION = 5.0  # Minimum segment length to keep\n\n# Transcription model - for new files without transcriptions\nTRANSCRIPTION_MODEL = \"distil-whisper/distil-large-v3\"\nTRANSCRIPTION_BATCH_SIZE = 8\n\n# Device\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nDTYPE = torch.bfloat16   # Use bfloat16 for training\n\n# Validation split\nVAL_SPLIT = 0.1  # 10% for validation\n\n# Create output directory\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(AUDIO_DIR, exist_ok=True)\n\nprint(f\"Audio directory: {AUDIO_DIR}\")\nprint(f\"Output directory: {OUTPUT_DIR}\")\nprint(f\"Device: {DEVICE}\")\nprint(f\"\\nControllable Rhythm Training: {'ENABLED' if USE_CONTENT_CONDITIONING else 'DISABLED'}\")\nprint(f\"\\nTraining config:\")\nprint(f\"  LoRA rank: {LORA_RANK}\")\nprint(f\"  Learning rate: {LEARNING_RATE}\")\nprint(f\"  Epochs: {NUM_EPOCHS}\")\nprint(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\nprint(f\"\\nLoRA targets: {len(TARGET_MODULES)} module patterns\")\nprint(f\"  (includes wk_latent, wv_latent for rhythm control)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from inference import (\n    load_model_from_hf,\n    load_fish_ae_from_hf,\n    load_pca_state_from_hf,\n    ae_decode,\n    get_speaker_latent_and_mask,\n    get_text_input_ids_and_mask,\n    get_content_latent,\n    sample_euler_cfg_independent_guidances,\n    sample_pipeline,\n    load_audio,\n)\nfrom functools import partial\n\nprint(\"Loading EchoDiT model...\")\nprint(\"NOTE: Keeping latent_encoder for controllable rhythm/timing!\")\nmodel = load_model_from_hf(\n    device=DEVICE,\n    dtype=DTYPE,\n    compile=False,  # Don't compile for training\n    delete_blockwise_modules=False,  # KEEP latent_encoder for rhythm control!\n)\nprint(f\"Model loaded: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M parameters\")\n\nprint(\"\\nLoading Fish-S1-DAC autoencoder...\")\nfish_ae = load_fish_ae_from_hf(\n    device=DEVICE,\n    dtype=torch.float32,  # AE needs float32 for quality\n)\nprint(\"Autoencoder loaded\")\n\nprint(\"\\nLoading PCA state...\")\npca_state = load_pca_state_from_hf(device=DEVICE)\nprint(\"PCA state loaded\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"All models loaded successfully!\")\nprint(\"Latent encoder available for controllable rhythm!\")\nprint(\"=\"*50)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Data Preparation\n\nWith 225 rap acapellas, we need to:\n1. List and validate all audio files\n2. Transcribe FULL SONGS using **Parakeet** (5-10x faster than Whisper!)\n   - Long songs are chunked into 25-second segments\n   - Each segment gets its own precise transcription\n   - This allows training on the ENTIRE song, not just intros!\n3. Create train/validation split (from all segments)\n4. Pre-encode all audio segments to latents (cached for fast training)"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 audio files in /content/dataset/\n",
      "\n",
      "‚ö†Ô∏è  No audio files found!\n",
      "Please add your rap acapella files to: /content/dataset/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# List all audio files\n",
    "audio_files = []\n",
    "for ext in [\".mp3\", \".wav\", \".flac\", \".m4a\", \".ogg\"]:\n",
    "    audio_files.extend(Path(AUDIO_DIR).glob(f\"**/*{ext}\"))\n",
    "\n",
    "print(f\"Found {len(audio_files)} audio files in {AUDIO_DIR}\")\n",
    "\n",
    "if len(audio_files) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  No audio files found!\")\n",
    "    print(f\"Please add your rap acapella files to: {AUDIO_DIR}\")\n",
    "else:\n",
    "    print(f\"\\nSample files:\")\n",
    "    for f in list(audio_files)[:5]:\n",
    "        print(f\"  {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom pathlib import Path\n\n# Transcribe all audio files using NVIDIA Parakeet (5-10x FASTER than Whisper!)\n# NOW WITH FULL SONG SUPPORT - chunks long audio and transcribes each segment!\n\nfrom train_utils import transcribe_audio_files_parakeet\nimport json\n\nTRANSCRIPTION_CACHE = os.path.join(OUTPUT_DIR, \"transcriptions.json\")\n\n# IMPORTANT: Delete old transcriptions.json to re-transcribe with full song support!\n# The old version only transcribed the first 30 seconds (intros)\n# The new version chunks the FULL song and transcribes each 25s segment\n\n# Check if we have cached transcriptions\nif os.path.exists(TRANSCRIPTION_CACHE):\n    print(f\"Loading cached transcriptions from {TRANSCRIPTION_CACHE}\")\n    with open(TRANSCRIPTION_CACHE, \"r\") as f:\n        transcriptions = json.load(f)\n    print(f\"Loaded {len(transcriptions)} cached transcriptions (segments)\")\n    print(f\"NOTE: If these are old transcriptions (only intros), delete the file and re-run this cell!\")\n    \n    # Find base files that need transcription (not counting segments)\n    base_files_transcribed = set()\n    for key in transcriptions.keys():\n        base_path = key.split(\"#segment_\")[0]\n        base_files_transcribed.add(base_path)\n    \n    files_to_transcribe = [f for f in audio_files if str(f) not in base_files_transcribed]\n    print(f\"Base files still needing transcription: {len(files_to_transcribe)}\")\nelse:\n    transcriptions = {}\n    files_to_transcribe = audio_files\n\nif len(files_to_transcribe) > 0:\n    print(f\"\\n{'='*60}\")\n    print(f\"FULL SONG TRANSCRIPTION WITH PARAKEET\")\n    print(f\"{'='*60}\")\n    print(f\"Files to transcribe: {len(files_to_transcribe)}\")\n    print(f\"\\nEach song will be:\")\n    print(f\"  - Chunked into 25-second segments (10% overlap)\")\n    print(f\"  - Each segment transcribed separately\")\n    print(f\"  - Result: FULL song coverage, not just intros!\")\n    print(f\"\\nEstimated time:\")\n    print(f\"  Whisper large-v3: ~30-45 minutes\")\n    print(f\"  Parakeet: ~10-15 minutes\")\n    print(f\"  Speedup: 3-5x faster!\\n\")\n    \n    # Use Parakeet with chunking\n    batch_transcriptions = transcribe_audio_files_parakeet(\n        audio_paths=[str(f) for f in files_to_transcribe],\n        model_name=TRANSCRIPTION_MODEL,\n        batch_size=TRANSCRIPTION_BATCH_SIZE,\n        chunk_duration=25.0,  # 25 second chunks\n        overlap=0.1,  # 10% overlap\n    )\n    \n    transcriptions.update(batch_transcriptions)\n    \n    # Save progress\n    with open(TRANSCRIPTION_CACHE, \"w\") as f:\n        json.dump(transcriptions, f, indent=2)\n    \n    print(f\"\\nSaved to: {TRANSCRIPTION_CACHE}\")\n\nprint(f\"\\n{'='*60}\")\nprint(f\"Transcription complete: {len(transcriptions)} segments\")\nprint(f\"{'='*60}\")\n\n# Count base files\nbase_files = set()\nfor key in transcriptions.keys():\n    base_path = key.split(\"#segment_\")[0]\n    base_files.add(base_path)\nprint(f\"Total files transcribed: {len(base_files)}\")\nprint(f\"Total segments created: {len(transcriptions)}\")\nprint(f\"Average segments per file: {len(transcriptions) / max(len(base_files), 1):.1f}\")\n\n# Show a few samples\nprint(f\"\\nSample transcriptions (first 3 segments):\")\nfor i, (path, text) in enumerate(list(transcriptions.items())[:3]):\n    if \"#segment_\" in path:\n        base_name = Path(path.split(\"#segment_\")[0]).name\n        segment_num = path.split(\"#segment_\")[1]\n        print(f\"\\n{base_name} [segment {segment_num}]:\")\n    else:\n        print(f\"\\n{Path(path).name}:\")\n    print(f\"  {text[:150]}{'...' if len(text) > 150 else ''}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Edit transcriptions manually if needed\n",
    "# Uncomment and modify as needed:\n",
    "\n",
    "# transcriptions[\"/path/to/file.mp3\"] = \"[S1] Your corrected transcription here.\"\n",
    "\n",
    "# Tips for transcriptions:\n",
    "# - Start with [S1] for single speaker\n",
    "# - Use commas for pauses\n",
    "# - Exclamation marks increase expressiveness\n",
    "# - Keep punctuation natural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset with train/val split\n",
    "from train_utils import (\n",
    "    TrainingSample,\n",
    "    EchoTTSDataset,\n",
    "    collate_fn,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "# Create training samples from transcriptions\n",
    "all_samples = []\n",
    "for path, text in transcriptions.items():\n",
    "    if text and len(text.strip()) > 10:  # Filter out empty/very short transcriptions\n",
    "        all_samples.append(TrainingSample(\n",
    "            audio_path=path,\n",
    "            text=text,\n",
    "            speaker_audio_path=None,  # Use same audio as speaker reference\n",
    "        ))\n",
    "\n",
    "print(f\"Created {len(all_samples)} training samples (filtered {len(transcriptions) - len(all_samples)} empty)\")\n",
    "\n",
    "if len(all_samples) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  ERROR: No valid training samples!\")\n",
    "    print(\"Please check that your audio files were transcribed correctly.\")\n",
    "else:\n",
    "    # Shuffle and split into train/val\n",
    "    random.seed(42)\n",
    "    random.shuffle(all_samples)\n",
    "\n",
    "    val_size = max(1, int(len(all_samples) * VAL_SPLIT))\n",
    "    train_samples = all_samples[val_size:]\n",
    "    val_samples = all_samples[:val_size]\n",
    "\n",
    "    print(f\"Train samples: {len(train_samples)}\")\n",
    "    print(f\"Validation samples: {len(val_samples)}\")\n",
    "\n",
    "    # Create datasets\n",
    "    print(\"\\nCreating training dataset and encoding audio to latents...\")\n",
    "    print(\"This will take a few minutes...\\n\")\n",
    "\n",
    "    train_dataset = EchoTTSDataset(\n",
    "        samples=train_samples,\n",
    "        fish_ae=fish_ae,\n",
    "        pca_state=pca_state,\n",
    "        device=DEVICE,\n",
    "        max_latent_length=MAX_LATENT_LENGTH,\n",
    "        cache_latents=True,\n",
    "    )\n",
    "\n",
    "    print(\"\\nCreating validation dataset...\")\n",
    "    val_dataset = EchoTTSDataset(\n",
    "        samples=val_samples,\n",
    "        fish_ae=fish_ae,\n",
    "        pca_state=pca_state,\n",
    "        device=DEVICE,\n",
    "        max_latent_length=MAX_LATENT_LENGTH,\n",
    "        cache_latents=True,\n",
    "    )\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Dataset ready!\")\n",
    "    print(f\"  Train: {len(train_dataset)} samples, {len(train_dataloader)} batches/epoch\")\n",
    "    print(f\"  Val: {len(val_dataset)} samples, {len(val_dataloader)} batches\")\n",
    "    print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply LoRA to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lora import (\n",
    "    apply_lora_to_model,\n",
    "    count_parameters,\n",
    "    get_lora_params,\n",
    "    save_lora_checkpoint,\n",
    "    load_lora_checkpoint,\n",
    ")\n",
    "\n",
    "# Apply LoRA adapters to the model\n",
    "print(\"Applying LoRA adapters...\")\n",
    "print(f\"  Rank: {LORA_RANK}\")\n",
    "print(f\"  Alpha: {LORA_ALPHA}\")\n",
    "print(f\"  Dropout: {LORA_DROPOUT}\")\n",
    "print(f\"  Target modules: {len(TARGET_MODULES)} patterns\")\n",
    "\n",
    "model, lora_modules = apply_lora_to_model(\n",
    "    model,\n",
    "    rank=LORA_RANK,\n",
    "    alpha=LORA_ALPHA,\n",
    "    dropout=LORA_DROPOUT,\n",
    "    target_modules=TARGET_MODULES,\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "print(f\"\\nParameter counts:\")\n",
    "print(f\"  Total: {total_params / 1e6:.1f}M\")\n",
    "print(f\"  Trainable (LoRA): {trainable_params / 1e6:.2f}M ({100 * trainable_params / total_params:.2f}%)\")\n",
    "print(f\"  LoRA modules applied: {len(lora_modules)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from train_utils import train_epoch, get_cosine_schedule_with_warmup, training_step\n\n# Setup optimizer (only LoRA params)\nlora_params = get_lora_params(model)\noptimizer = torch.optim.AdamW(\n    lora_params,\n    lr=LEARNING_RATE,\n    weight_decay=0.01,\n    betas=(0.9, 0.999),\n)\n\n# Learning rate scheduler\nnum_training_steps = len(train_dataloader) * NUM_EPOCHS // GRADIENT_ACCUMULATION\nscheduler = get_cosine_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=min(WARMUP_STEPS, num_training_steps // 10),\n    num_training_steps=num_training_steps,\n)\n\n# Mixed precision scaler\nscaler = torch.cuda.amp.GradScaler()\n\nprint(\"Training setup:\")\nprint(f\"  Epochs: {NUM_EPOCHS}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION}\")\nprint(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\nprint(f\"  Learning rate: {LEARNING_RATE}\")\nprint(f\"  Training steps: {num_training_steps}\")\nprint(f\"  Warmup steps: {min(WARMUP_STEPS, num_training_steps // 10)}\")\nprint(f\"  Train batches/epoch: {len(train_dataloader)}\")\nprint(f\"  Val batches: {len(val_dataloader)}\")\nprint(f\"\\n  Controllable Rhythm: {'ENABLED' if USE_CONTENT_CONDITIONING else 'DISABLED'}\")\n\n# Validation function\n@torch.no_grad()\ndef validate(model, val_dataloader, device, use_content_conditioning=True):\n    \"\"\"Validation loop using train_utils.training_step\"\"\"\n    model.eval()\n    total_loss = 0.0\n    num_batches = 0\n    \n    for batch in val_dataloader:\n        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n            loss = training_step(model, batch, device, use_content_conditioning=use_content_conditioning)\n        \n        # Skip NaN losses in validation too\n        if not torch.isnan(loss) and not torch.isinf(loss):\n            total_loss += loss.item()\n            num_batches += 1\n    \n    model.train()\n    \n    if num_batches == 0:\n        return float('nan')\n    \n    return total_loss / num_batches"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training loop with validation and controllable rhythm\nprint(\"\\n\" + \"=\"*50)\nprint(\"Starting training with CONTROLLABLE RHYTHM...\")\nprint(\"=\"*50 + \"\\n\")\n\nhistory = {\"train_loss\": [], \"val_loss\": [], \"epoch\": [], \"lr\": []}\nbest_val_loss = float(\"inf\")\n\nfor epoch in range(NUM_EPOCHS):\n    # Train one epoch with content conditioning for rhythm learning\n    train_loss = train_epoch(\n        model=model,\n        dataloader=train_dataloader,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        device=DEVICE,\n        gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n        max_grad_norm=MAX_GRAD_NORM,\n        scaler=scaler,\n        use_content_conditioning=USE_CONTENT_CONDITIONING,  # Controllable rhythm!\n    )\n    \n    # Validate\n    val_loss = validate(model, val_dataloader, DEVICE, use_content_conditioning=USE_CONTENT_CONDITIONING)\n    \n    # Get current LR\n    current_lr = scheduler.get_last_lr()[0]\n    \n    # Record history\n    history[\"train_loss\"].append(train_loss)\n    history[\"val_loss\"].append(val_loss)\n    history[\"epoch\"].append(epoch + 1)\n    history[\"lr\"].append(current_lr)\n    \n    # Print progress\n    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} - Train: {train_loss:.4f} - Val: {val_loss:.4f} - LR: {current_lr:.2e}\")\n    \n    # Save best checkpoint (based on validation loss)\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        save_lora_checkpoint(\n            model,\n            os.path.join(OUTPUT_DIR, \"lora_best.pt\"),\n            config={\n                \"rank\": LORA_RANK,\n                \"alpha\": LORA_ALPHA,\n                \"dropout\": LORA_DROPOUT,\n                \"target_modules\": TARGET_MODULES,\n                \"epoch\": epoch + 1,\n                \"train_loss\": train_loss,\n                \"val_loss\": val_loss,\n                \"use_content_conditioning\": USE_CONTENT_CONDITIONING,\n            }\n        )\n        print(f\"  -> Saved best checkpoint (val_loss: {best_val_loss:.4f})\")\n    \n    # Periodic checkpoint every 2 epochs\n    if (epoch + 1) % 2 == 0:\n        save_lora_checkpoint(\n            model,\n            os.path.join(OUTPUT_DIR, f\"lora_epoch_{epoch + 1}.pt\"),\n            config={\n                \"rank\": LORA_RANK,\n                \"alpha\": LORA_ALPHA,\n                \"dropout\": LORA_DROPOUT,\n                \"target_modules\": TARGET_MODULES,\n                \"epoch\": epoch + 1,\n                \"train_loss\": train_loss,\n                \"val_loss\": val_loss,\n                \"use_content_conditioning\": USE_CONTENT_CONDITIONING,\n            }\n        )\n\n# Save final checkpoint\nsave_lora_checkpoint(\n    model,\n    os.path.join(OUTPUT_DIR, \"lora_final.pt\"),\n    config={\n        \"rank\": LORA_RANK,\n        \"alpha\": LORA_ALPHA,\n        \"dropout\": LORA_DROPOUT,\n        \"target_modules\": TARGET_MODULES,\n        \"epoch\": NUM_EPOCHS,\n        \"train_loss\": history[\"train_loss\"][-1],\n        \"val_loss\": history[\"val_loss\"][-1],\n        \"use_content_conditioning\": USE_CONTENT_CONDITIONING,\n    }\n)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Training complete!\")\nprint(f\"Best validation loss: {best_val_loss:.4f}\")\nprint(f\"Checkpoints saved to: {OUTPUT_DIR}\")\nprint(f\"Controllable rhythm: {'ENABLED' if USE_CONTENT_CONDITIONING else 'DISABLED'}\")\nprint(\"=\"*50)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Training and validation loss\n",
    "axes[0].plot(history[\"epoch\"], history[\"train_loss\"], 'b-', linewidth=2, label='Train')\n",
    "axes[0].plot(history[\"epoch\"], history[\"val_loss\"], 'r--', linewidth=2, label='Val')\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Training vs Validation Loss\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Training loss only (zoomed)\n",
    "axes[1].plot(history[\"epoch\"], history[\"train_loss\"], 'b-', linewidth=2)\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Loss\")\n",
    "axes[1].set_title(\"Training Loss\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[2].plot(history[\"epoch\"], history[\"lr\"], 'g-', linewidth=2)\n",
    "axes[2].set_xlabel(\"Epoch\")\n",
    "axes[2].set_ylabel(\"Learning Rate\")\n",
    "axes[2].set_title(\"Learning Rate Schedule\")\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"training_curves.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Final train loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"  Final val loss: {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"  Best val loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation & Inference\n",
    "\n",
    "Generate samples with your fine-tuned model and compare to the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Test prompts - rap style!\n",
    "TEST_PROMPTS = [\n",
    "    \"[S1] Yeah, I'm spitting fire on the mic tonight, gonna show them what I got, rising to the top!\",\n",
    "    \"[S1] The rhythm flows through me like water, every beat hits harder, I'm a natural born starter!\",\n",
    "    \"[S1] Check it out, I'm the one they've been waiting for, coming through the door, ready to explore!\",\n",
    "    \"[S1] Money on my mind, grind never stops, from the bottom to the top, watch me drop!\",\n",
    "    \"[S1] Real recognize real, that's the deal, keep it trill, got the skill to make you feel!\",\n",
    "]\n",
    "\n",
    "# Use a random training file as speaker reference\n",
    "import random\n",
    "SPEAKER_AUDIO_PATH = random.choice(audio_files) if audio_files else None\n",
    "\n",
    "print(f\"Using speaker reference: {SPEAKER_AUDIO_PATH}\")\n",
    "print(f\"\\nWill generate {len(TEST_PROMPTS)} samples...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate samples with fine-tuned model - NOW WITH CONTROLLABLE RHYTHM!\n@torch.inference_mode()\ndef generate_audio(model, text, speaker_audio_path=None, content_audio_path=None, seed=0):\n    \"\"\"\n    Generate audio using the (fine-tuned) model.\n    \n    Args:\n        model: Fine-tuned EchoDiT model\n        text: Text to speak\n        speaker_audio_path: Audio for voice identity (WHO it sounds like)\n        content_audio_path: Audio for rhythm/timing (HOW it's delivered) - NEW!\n        seed: Random seed for reproducibility\n    \"\"\"\n    \n    # Load speaker audio (for voice identity)\n    if speaker_audio_path:\n        speaker_audio = load_audio(speaker_audio_path)\n    else:\n        speaker_audio = None\n    \n    # Load content audio (for rhythm/timing) - CONTROLLABLE!\n    if content_audio_path:\n        content_audio = load_audio(content_audio_path)\n    else:\n        content_audio = None\n    \n    # Create sample function with content_latent support\n    sample_fn = partial(\n        sample_euler_cfg_independent_guidances,\n        num_steps=40,\n        cfg_scale_text=3.0,\n        cfg_scale_speaker=8.0,\n        cfg_min_t=0.5,\n        cfg_max_t=1.0,\n        truncation_factor=0.8,\n        rescale_k=None,\n        rescale_sigma=None,\n        speaker_kv_scale=None,\n        speaker_kv_max_layers=None,\n        speaker_kv_min_t=None,\n        sequence_length=640,\n    )\n    \n    # Generate with both speaker and content references\n    audio_out, normalized_text = sample_pipeline(\n        model=model,\n        fish_ae=fish_ae,\n        pca_state=pca_state,\n        sample_fn=sample_fn,\n        text_prompt=text,\n        speaker_audio=speaker_audio,\n        rng_seed=seed,\n        content_audio=content_audio,  # Controllable rhythm!\n    )\n    \n    return audio_out[0].cpu(), normalized_text\n\n# Generate and play samples\nprint(\"Generating samples with fine-tuned model...\\n\")\nprint(\"Using CONTROLLABLE RHYTHM - speaker for voice, content for timing!\\n\")\n\nfor i, prompt in enumerate(TEST_PROMPTS):\n    print(f\"Prompt {i + 1}: {prompt}\")\n    \n    # Use same audio for both speaker and content (for testing)\n    # In practice, you can use DIFFERENT audio for content to transfer rhythm!\n    audio, _ = generate_audio(\n        model,\n        prompt,\n        speaker_audio_path=str(SPEAKER_AUDIO_PATH) if SPEAKER_AUDIO_PATH else None,\n        content_audio_path=str(SPEAKER_AUDIO_PATH) if SPEAKER_AUDIO_PATH else None,  # Can be different!\n        seed=i,\n    )\n    \n    # Save audio\n    output_path = os.path.join(OUTPUT_DIR, f\"sample_{i + 1}.wav\")\n    torchaudio.save(output_path, audio.unsqueeze(0), 44100)\n    print(f\"Saved to: {output_path}\")\n    \n    # Play audio\n    display(Audio(audio.numpy(), rate=44100))\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Checkpoint for Later Use\n",
    "\n",
    "Use this section to load a saved LoRA checkpoint onto a fresh model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load a saved LoRA checkpoint\n",
    "# Uncomment and run this cell to load a checkpoint\n",
    "\n",
    "# CHECKPOINT_PATH = os.path.join(OUTPUT_DIR, \"lora_best.pt\")\n",
    "# \n",
    "# # Load fresh base model\n",
    "# model_fresh = load_model_from_hf(\n",
    "#     device=DEVICE,\n",
    "#     dtype=DTYPE,\n",
    "#     compile=False,\n",
    "#     delete_blockwise_modules=True,\n",
    "# )\n",
    "# \n",
    "# # Load checkpoint to get config\n",
    "# checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "# config = checkpoint[\"config\"]\n",
    "# \n",
    "# # Apply LoRA with saved config\n",
    "# model_fresh, _ = apply_lora_to_model(\n",
    "#     model_fresh,\n",
    "#     rank=config[\"rank\"],\n",
    "#     alpha=config[\"alpha\"],\n",
    "#     dropout=0.0,  # No dropout for inference\n",
    "#     target_modules=config[\"target_modules\"],\n",
    "# )\n",
    "# \n",
    "# # Load LoRA weights\n",
    "# load_lora_checkpoint(model_fresh, CHECKPOINT_PATH, device=DEVICE)\n",
    "# model_fresh.eval()\n",
    "# \n",
    "# print(f\"Loaded checkpoint from epoch {config['epoch']} (val_loss: {config['val_loss']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Tips & Notes for Controllable Rap Training\n\n### NEW: Controllable Rhythm/Timing\nThis training enables **separate control** over:\n- **Speaker Reference** ‚Üí Voice identity (timbre, pitch, who it sounds like)\n- **Content Reference** ‚Üí Rhythm/timing (pacing, flow, cadence)\n\n**Use Cases:**\n- Same lyrics with Eminem's flow vs Drake's flow\n- Transfer a specific verse's rhythm to any voice\n- Keep your voice but rap like someone else\n\n### What We Configured\n- **LoRA rank 32**: Higher rank captures more style nuance\n- **Content conditioning**: Enabled for rhythm learning\n- **Latent path training**: `wk_latent`, `wv_latent` included in targets\n- **Parakeet transcription**: 5-10x faster than Whisper\n\n### Training Expectations\nWith 173+ rap acapellas:\n- **Training time**: ~2-4 hours on T4, ~1-2 hours on A100\n- **Expected final loss**: ~0.05-0.15 (lower is better)\n- **Checkpoint size**: ~80-100MB\n\n### If Results Sound Off\n1. **Rhythm not transferring**: Make sure `USE_CONTENT_CONDITIONING = True`\n2. **Too monotone**: Increase `cfg_scale_text` to 4-5 during inference\n3. **Voice doesn't match**: Try different speaker reference audio\n4. **Gibberish output**: Check if transcriptions were accurate\n\n### How to Use at Inference\n```python\n# Same text, DIFFERENT rhythm sources:\naudio1, _ = generate_audio(\n    model,\n    text=\"Your rap lyrics here\",\n    speaker_audio_path=\"your_voice.wav\",      # WHO it sounds like\n    content_audio_path=\"fast_flow_ref.wav\",   # HOW it's delivered (fast)\n)\n\naudio2, _ = generate_audio(\n    model,\n    text=\"Your rap lyrics here\",\n    speaker_audio_path=\"your_voice.wav\",      # Same voice\n    content_audio_path=\"slow_flow_ref.wav\",   # Different rhythm (slow)\n)\n```\n\n### Common Issues\n\n**Out of Memory (OOM)**:\n- Reduce `MAX_LATENT_LENGTH` to 320 (15 seconds max)\n- Use A100 GPU on Colab instead of T4\n- Reduce `LORA_RANK` to 16\n\n**Loss not decreasing**:\n- Check transcriptions are accurate\n- Ensure you have enough training data\n\n**Validation loss increasing (overfitting)**:\n- Increase `LORA_DROPOUT` to 0.1\n- Reduce `NUM_EPOCHS`\n- Reduce `LORA_RANK` to 16\n\n### Voice Cloning Still Works!\nThe speaker path (wk_speaker, wv_speaker) was kept frozen, so you can:\n- Use ANY speaker reference audio at inference time\n- The rap style AND rhythm control transfer to any voice\n- Original voice cloning quality is preserved"
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Interactive Testing with Gradio\n\nLaunch a Gradio interface to test your fine-tuned rap model interactively!",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}