{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Echo-TTS Rap Style Fine-Tuning\n",
    "\n",
    "This notebook implements LoRA-based fine-tuning for Echo-TTS to adapt the model to **rapping style** while preserving voice cloning capabilities.\n",
    "\n",
    "**Dataset:**\n",
    "- 225 rap acapellas (unprocessed)\n",
    "- Requires preprocessing: segmentation, transcription\n",
    "\n",
    "**Requirements:**\n",
    "- GPU with 16GB+ VRAM (T4/A100 on Colab)\n",
    "\n",
    "**What you'll get:**\n",
    "- Fine-tuned model that generates rap-style speech\n",
    "- LoRA checkpoint (~50-100MB) that can be loaded on top of base model\n",
    "- Preserved voice cloning - can still use any speaker reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the echo-tts repository\n",
    "!rm -rf /content/echo-tts\n",
    "!git clone https://github.com/CoreBedtime/echo-tts.git 2>/dev/null || echo \"Repo already exists\"\n",
    "\n",
    "# Add to Python path\n",
    "import sys\n",
    "sys.path.insert(0, '/content/echo-tts')\n",
    "\n",
    "# Change working directory\n",
    "import os\n",
    "os.chdir('/content/echo-tts')\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q torch torchaudio safetensors huggingface-hub einops\n",
    "!pip install -q openai-whisper  # For automatic transcription\n",
    "!pip install -q torchcodec  # For audio decoding\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchaudio safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for saving checkpoints (Colab)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# =============================================================================\n",
    "# PATHS - Update these!\n",
    "# =============================================================================\n",
    "\n",
    "# Directory containing your training audio files\n",
    "AUDIO_DIR = \"/content/dataset/\"  # Put your rap acapellas here\n",
    "\n",
    "# Where to save checkpoints\n",
    "OUTPUT_DIR = \"./checkpoints/\" \n",
    "\n",
    "# =============================================================================\n",
    "# LoRA CONFIGURATION (optimized for 225 samples)\n",
    "# =============================================================================\n",
    "\n",
    "LORA_RANK = 32          # Higher rank for larger dataset (more expressiveness)\n",
    "LORA_ALPHA = 32.0       # Scaling factor, typically equal to rank\n",
    "LORA_DROPOUT = 0.05     # Lower dropout - more data means less regularization needed\n",
    "\n",
    "# Which modules to train (default preserves voice cloning path)\n",
    "# Speaker path (wk_speaker, wv_speaker) is NOT trained to preserve cloning\n",
    "TARGET_MODULES = [\n",
    "    # Main decoder attention (style)\n",
    "    \"blocks.*.attention.wq\",\n",
    "    \"blocks.*.attention.wk\",\n",
    "    \"blocks.*.attention.wv\",\n",
    "    \"blocks.*.attention.wo\",\n",
    "    # Text cross-attention (text-to-audio mapping)\n",
    "    \"blocks.*.attention.wk_text\",\n",
    "    \"blocks.*.attention.wv_text\",\n",
    "    # MLP layers (feature transformation)\n",
    "    \"blocks.*.mlp.w1\",\n",
    "    \"blocks.*.mlp.w2\",\n",
    "    \"blocks.*.mlp.w3\",\n",
    "]\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING CONFIGURATION (optimized for 225 samples)\n",
    "# =============================================================================\n",
    "\n",
    "LEARNING_RATE = 1e-4     # Slightly higher LR for larger dataset\n",
    "NUM_EPOCHS = 10          # Fewer epochs needed with more data\n",
    "BATCH_SIZE = 1           # Batch size (1 for memory efficiency)\n",
    "GRADIENT_ACCUMULATION = 8  # Larger effective batch for stability\n",
    "MAX_GRAD_NORM = 1.0      # Gradient clipping\n",
    "WARMUP_STEPS = 100       # More warmup steps for larger dataset\n",
    "\n",
    "# Audio settings\n",
    "MAX_LATENT_LENGTH = 640  # Max ~30 seconds (reduce to 320 if OOM)\n",
    "SEGMENT_DURATION = 25.0  # Split long audio into ~25 second chunks\n",
    "MIN_SEGMENT_DURATION = 5.0  # Minimum segment length to keep\n",
    "\n",
    "# Whisper model for transcription - LARGE-V3 for best rap lyrics accuracy!\n",
    "WHISPER_MODEL = \"large-v3\"  # Options: tiny, base, small, medium, large-v3\n",
    "\n",
    "# Parallel transcription settings\n",
    "NUM_TRANSCRIPTION_WORKERS = 4  # Number of parallel workers (adjust based on CPU/GPU)\n",
    "TRANSCRIPTION_BATCH_SIZE = 8   # Files per batch for progress updates\n",
    "\n",
    "# Device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.bfloat16   # Use bfloat16 for training\n",
    "\n",
    "# Validation split\n",
    "VAL_SPLIT = 0.1  # 10% for validation (~22 samples)\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(AUDIO_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Audio directory: {AUDIO_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"\\nTraining config for 225 samples:\")\n",
    "print(f\"  LoRA rank: {LORA_RANK}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"\\nTranscription config:\")\n",
    "print(f\"  Whisper model: {WHISPER_MODEL}\")\n",
    "print(f\"  Parallel workers: {NUM_TRANSCRIPTION_WORKERS}\")\n",
    "print(f\"  This will be MUCH faster than sequential!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import (\n",
    "    load_model_from_hf,\n",
    "    load_fish_ae_from_hf,\n",
    "    load_pca_state_from_hf,\n",
    "    ae_decode,\n",
    "    get_speaker_latent_and_mask,\n",
    "    get_text_input_ids_and_mask,\n",
    "    sample_euler_cfg_independent_guidances,\n",
    "    sample_pipeline,\n",
    "    load_audio,\n",
    ")\n",
    "from functools import partial\n",
    "\n",
    "print(\"Loading EchoDiT model...\")\n",
    "model = load_model_from_hf(\n",
    "    device=DEVICE,\n",
    "    dtype=DTYPE,\n",
    "    compile=False,  # Don't compile for training\n",
    "    delete_blockwise_modules=True,  # Save memory\n",
    ")\n",
    "print(f\"Model loaded: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M parameters\")\n",
    "\n",
    "print(\"\\nLoading Fish-S1-DAC autoencoder...\")\n",
    "fish_ae = load_fish_ae_from_hf(\n",
    "    device=DEVICE,\n",
    "    dtype=torch.float32,  # AE needs float32 for quality\n",
    ")\n",
    "print(\"Autoencoder loaded\")\n",
    "\n",
    "print(\"\\nLoading PCA state...\")\n",
    "pca_state = load_pca_state_from_hf(device=DEVICE)\n",
    "print(\"PCA state loaded\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"All models loaded successfully!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation\n",
    "\n",
    "With 225 rap acapellas, we need to:\n",
    "1. List and validate all audio files\n",
    "2. Segment long tracks into training-sized chunks (~25 seconds)\n",
    "3. Transcribe using Whisper (medium model for better rap lyrics)\n",
    "4. Create train/validation split\n",
    "5. Pre-encode all audio to latents (cached for fast training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# List all audio files\n",
    "audio_files = []\n",
    "for ext in [\".mp3\", \".wav\", \".flac\", \".m4a\", \".ogg\"]:\n",
    "    audio_files.extend(Path(AUDIO_DIR).glob(f\"**/*{ext}\"))\n",
    "\n",
    "print(f\"Found {len(audio_files)} audio files in {AUDIO_DIR}\")\n",
    "\n",
    "if len(audio_files) == 0:\n",
    "    print(\"\\n⚠️  No audio files found!\")\n",
    "    print(f\"Please add your rap acapella files to: {AUDIO_DIR}\")\n",
    "else:\n",
    "    print(f\"\\nSample files:\")\n",
    "    for f in list(audio_files)[:5]:\n",
    "        print(f\"  {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Transcribe all audio files using Whisper LARGE-V3 in PARALLEL\n",
    "# This will be MUCH faster than sequential processing!\n",
    "\n",
    "from train_utils import transcribe_audio_files_parallel\n",
    "import json\n",
    "\n",
    "TRANSCRIPTION_CACHE = os.path.join(OUTPUT_DIR, \"transcriptions.json\")\n",
    "\n",
    "# Check if we have cached transcriptions\n",
    "if os.path.exists(TRANSCRIPTION_CACHE):\n",
    "    print(f\"Loading cached transcriptions from {TRANSCRIPTION_CACHE}\")\n",
    "    with open(TRANSCRIPTION_CACHE, \"r\") as f:\n",
    "        transcriptions = json.load(f)\n",
    "    print(f\"Loaded {len(transcriptions)} cached transcriptions\")\n",
    "    \n",
    "    # Find files that still need transcription\n",
    "    cached_paths = set(transcriptions.keys())\n",
    "    files_to_transcribe = [f for f in audio_files if str(f) not in cached_paths]\n",
    "    print(f\"Files still needing transcription: {len(files_to_transcribe)}\")\n",
    "else:\n",
    "    transcriptions = {}\n",
    "    files_to_transcribe = audio_files\n",
    "\n",
    "if len(files_to_transcribe) > 0:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PARALLEL TRANSCRIPTION with Whisper {WHISPER_MODEL}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Files to transcribe: {len(files_to_transcribe)}\")\n",
    "    print(f\"Workers: {NUM_TRANSCRIPTION_WORKERS}\")\n",
    "    print(f\"\\nEstimated time:\")\n",
    "    print(f\"  Sequential (old): ~{len(files_to_transcribe) * 0.5:.0f} minutes\")\n",
    "    print(f\"  Parallel (new): ~{len(files_to_transcribe) * 0.5 / NUM_TRANSCRIPTION_WORKERS:.0f} minutes\")\n",
    "    print(f\"  Speedup: {NUM_TRANSCRIPTION_WORKERS}x faster!\\n\")\n",
    "    \n",
    "    # Use parallel transcription (MUCH faster!)\n",
    "    batch_transcriptions = transcribe_audio_files_parallel(\n",
    "        audio_paths=[str(f) for f in files_to_transcribe],\n",
    "        model_name=WHISPER_MODEL,\n",
    "        language=\"en\",\n",
    "        num_workers=NUM_TRANSCRIPTION_WORKERS,\n",
    "        batch_size=TRANSCRIPTION_BATCH_SIZE,\n",
    "    )\n",
    "    \n",
    "    transcriptions.update(batch_transcriptions)\n",
    "    \n",
    "    # Save progress\n",
    "    with open(TRANSCRIPTION_CACHE, \"w\") as f:\n",
    "        json.dump(transcriptions, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nSaved to: {TRANSCRIPTION_CACHE}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Transcription complete: {len(transcriptions)} files\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Show a few samples\n",
    "print(f\"\\nSample transcriptions:\")\n",
    "for i, (path, text) in enumerate(list(transcriptions.items())[:3]):\n",
    "    filename = Path(path).name\n",
    "    print(f\"\\n{filename}:\")\n",
    "    print(f\"  {text[:150]}{'...' if len(text) > 150 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Edit transcriptions manually if Whisper made mistakes\n",
    "# Uncomment and modify as needed:\n",
    "\n",
    "# transcriptions[\"/path/to/file.mp3\"] = \"[S1] Your corrected transcription here.\"\n",
    "\n",
    "# Tips for transcriptions:\n",
    "# - Start with [S1] for single speaker\n",
    "# - Use commas for pauses\n",
    "# - Exclamation marks increase expressiveness\n",
    "# - Keep punctuation natural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset with train/val split\n",
    "from train_utils import (\n",
    "    TrainingSample,\n",
    "    EchoTTSDataset,\n",
    "    collate_fn,\n",
    "    segment_audio,\n",
    "    load_audio_tensor,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "# Create training samples\n",
    "all_samples = []\n",
    "for path, text in transcriptions.items():\n",
    "    if text and len(text.strip()) > 10:  # Filter out empty/very short transcriptions\n",
    "        all_samples.append(TrainingSample(\n",
    "            audio_path=path,\n",
    "            text=text,\n",
    "            speaker_audio_path=None,  # Use same audio as speaker reference\n",
    "        ))\n",
    "\n",
    "print(f\"Created {len(all_samples)} training samples (filtered {len(transcriptions) - len(all_samples)} empty)\")\n",
    "\n",
    "# Shuffle and split into train/val\n",
    "random.seed(42)\n",
    "random.shuffle(all_samples)\n",
    "\n",
    "val_size = max(1, int(len(all_samples) * VAL_SPLIT))\n",
    "train_samples = all_samples[val_size:]\n",
    "val_samples = all_samples[:val_size]\n",
    "\n",
    "print(f\"Train samples: {len(train_samples)}\")\n",
    "print(f\"Validation samples: {len(val_samples)}\")\n",
    "\n",
    "# Create datasets\n",
    "print(\"\\nCreating training dataset and encoding audio to latents...\")\n",
    "print(\"This will take a few minutes for 225 files...\\n\")\n",
    "\n",
    "train_dataset = EchoTTSDataset(\n",
    "    samples=train_samples,\n",
    "    fish_ae=fish_ae,\n",
    "    pca_state=pca_state,\n",
    "    device=DEVICE,\n",
    "    max_latent_length=MAX_LATENT_LENGTH,\n",
    "    cache_latents=True,\n",
    ")\n",
    "\n",
    "print(\"\\nCreating validation dataset...\")\n",
    "val_dataset = EchoTTSDataset(\n",
    "    samples=val_samples,\n",
    "    fish_ae=fish_ae,\n",
    "    pca_state=pca_state,\n",
    "    device=DEVICE,\n",
    "    max_latent_length=MAX_LATENT_LENGTH,\n",
    "    cache_latents=True,\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Dataset ready!\")\n",
    "print(f\"  Train: {len(train_dataset)} samples, {len(train_dataloader)} batches/epoch\")\n",
    "print(f\"  Val: {len(val_dataset)} samples, {len(val_dataloader)} batches\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply LoRA to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lora import (\n",
    "    apply_lora_to_model,\n",
    "    count_parameters,\n",
    "    get_lora_params,\n",
    "    save_lora_checkpoint,\n",
    "    load_lora_checkpoint,\n",
    ")\n",
    "\n",
    "# Apply LoRA adapters to the model\n",
    "print(\"Applying LoRA adapters...\")\n",
    "print(f\"  Rank: {LORA_RANK}\")\n",
    "print(f\"  Alpha: {LORA_ALPHA}\")\n",
    "print(f\"  Dropout: {LORA_DROPOUT}\")\n",
    "print(f\"  Target modules: {len(TARGET_MODULES)} patterns\")\n",
    "\n",
    "model, lora_modules = apply_lora_to_model(\n",
    "    model,\n",
    "    rank=LORA_RANK,\n",
    "    alpha=LORA_ALPHA,\n",
    "    dropout=LORA_DROPOUT,\n",
    "    target_modules=TARGET_MODULES,\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "print(f\"\\nParameter counts:\")\n",
    "print(f\"  Total: {total_params / 1e6:.1f}M\")\n",
    "print(f\"  Trainable (LoRA): {trainable_params / 1e6:.2f}M ({100 * trainable_params / total_params:.2f}%)\")\n",
    "print(f\"  LoRA modules applied: {len(lora_modules)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_utils import train_epoch, get_cosine_schedule_with_warmup, training_step\n",
    "\n",
    "# Setup optimizer (only LoRA params)\n",
    "lora_params = get_lora_params(model)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    lora_params,\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    betas=(0.9, 0.999),\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "num_training_steps = len(train_dataloader) * NUM_EPOCHS // GRADIENT_ACCUMULATION\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=min(WARMUP_STEPS, num_training_steps // 10),\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "print(\"Training setup:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Training steps: {num_training_steps}\")\n",
    "print(f\"  Warmup steps: {min(WARMUP_STEPS, num_training_steps // 10)}\")\n",
    "print(f\"  Train batches/epoch: {len(train_dataloader)}\")\n",
    "print(f\"  Val batches: {len(val_dataloader)}\")\n",
    "\n",
    "# Validation function\n",
    "@torch.no_grad()\n",
    "def validate(model, val_dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in val_dataloader:\n",
    "        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "            loss = training_step(model, batch, device)\n",
    "        \n",
    "        # Skip NaN losses in validation too\n",
    "        if not torch.isnan(loss) and not torch.isinf(loss):\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    if num_batches == 0:\n",
    "        return float('nan')\n",
    "    \n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Test a single training step to diagnose NaN issues\n",
    "print(\"Running diagnostic training step...\")\n",
    "\n",
    "# Get a single batch\n",
    "test_batch = next(iter(train_dataloader))\n",
    "\n",
    "# Check batch data\n",
    "print(\"\\nBatch data statistics:\")\n",
    "print(f\"  Latent shape: {test_batch['latent'].shape}\")\n",
    "print(f\"  Latent range: [{test_batch['latent'].min():.4f}, {test_batch['latent'].max():.4f}]\")\n",
    "print(f\"  Latent has NaN: {torch.isnan(test_batch['latent']).any()}\")\n",
    "print(f\"  Speaker latent shape: {test_batch['speaker_latent'].shape}\")\n",
    "print(f\"  Speaker latent range: [{test_batch['speaker_latent'].min():.4f}, {test_batch['speaker_latent'].max():.4f}]\")\n",
    "print(f\"  Speaker latent has NaN: {torch.isnan(test_batch['speaker_latent']).any()}\")\n",
    "print(f\"  Texts: {test_batch['text']}\")\n",
    "\n",
    "# Try forward pass\n",
    "try:\n",
    "    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        loss = training_step(model, test_batch, DEVICE)\n",
    "    print(f\"\\nTest loss: {loss.item():.4f}\")\n",
    "    print(\"✓ Forward pass successful!\")\n",
    "    \n",
    "    # Try backward pass\n",
    "    loss.backward()\n",
    "    print(\"✓ Backward pass successful!\")\n",
    "    \n",
    "    # Check gradients\n",
    "    grad_norms = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            grad_norms.append((name, grad_norm))\n",
    "            if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
    "                print(f\"  ⚠️  NaN/Inf gradient in {name}\")\n",
    "    \n",
    "    if grad_norms:\n",
    "        # Sort by gradient magnitude\n",
    "        grad_norms.sort(key=lambda x: x[1], reverse=True)\n",
    "        print(f\"\\nTop 5 gradient magnitudes:\")\n",
    "        for name, norm in grad_norms[:5]:\n",
    "            print(f\"  {name}: {norm:.4f}\")\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error during test step: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with validation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"epoch\": [], \"lr\": []}\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train one epoch (scheduler is now passed and called inside train_epoch)\n",
    "    train_loss = train_epoch(\n",
    "        model=model,\n",
    "        dataloader=train_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,  # Pass scheduler to training loop\n",
    "        device=DEVICE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "        max_grad_norm=MAX_GRAD_NORM,\n",
    "        scaler=scaler,\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = validate(model, val_dataloader, DEVICE)\n",
    "    \n",
    "    # Get current LR (scheduler is stepped inside train_epoch now)\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    \n",
    "    # Record history\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"epoch\"].append(epoch + 1)\n",
    "    history[\"lr\"].append(current_lr)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} - Train: {train_loss:.4f} - Val: {val_loss:.4f} - LR: {current_lr:.2e}\")\n",
    "    \n",
    "    # Save best checkpoint (based on validation loss)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        save_lora_checkpoint(\n",
    "            model,\n",
    "            os.path.join(OUTPUT_DIR, \"lora_best.pt\"),\n",
    "            config={\n",
    "                \"rank\": LORA_RANK,\n",
    "                \"alpha\": LORA_ALPHA,\n",
    "                \"dropout\": LORA_DROPOUT,\n",
    "                \"target_modules\": TARGET_MODULES,\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "            }\n",
    "        )\n",
    "        print(f\"  -> Saved best checkpoint (val_loss: {best_val_loss:.4f})\")\n",
    "    \n",
    "    # Periodic checkpoint every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        save_lora_checkpoint(\n",
    "            model,\n",
    "            os.path.join(OUTPUT_DIR, f\"lora_epoch_{epoch + 1}.pt\"),\n",
    "            config={\n",
    "                \"rank\": LORA_RANK,\n",
    "                \"alpha\": LORA_ALPHA,\n",
    "                \"dropout\": LORA_DROPOUT,\n",
    "                \"target_modules\": TARGET_MODULES,\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Save final checkpoint\n",
    "save_lora_checkpoint(\n",
    "    model,\n",
    "    os.path.join(OUTPUT_DIR, \"lora_final.pt\"),\n",
    "    config={\n",
    "        \"rank\": LORA_RANK,\n",
    "        \"alpha\": LORA_ALPHA,\n",
    "        \"dropout\": LORA_DROPOUT,\n",
    "        \"target_modules\": TARGET_MODULES,\n",
    "        \"epoch\": NUM_EPOCHS,\n",
    "        \"train_loss\": history[\"train_loss\"][-1],\n",
    "        \"val_loss\": history[\"val_loss\"][-1],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training complete!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Checkpoints saved to: {OUTPUT_DIR}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Training and validation loss\n",
    "axes[0].plot(history[\"epoch\"], history[\"train_loss\"], 'b-', linewidth=2, label='Train')\n",
    "axes[0].plot(history[\"epoch\"], history[\"val_loss\"], 'r--', linewidth=2, label='Val')\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Training vs Validation Loss\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Training loss only (zoomed)\n",
    "axes[1].plot(history[\"epoch\"], history[\"train_loss\"], 'b-', linewidth=2)\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Loss\")\n",
    "axes[1].set_title(\"Training Loss\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[2].plot(history[\"epoch\"], history[\"lr\"], 'g-', linewidth=2)\n",
    "axes[2].set_xlabel(\"Epoch\")\n",
    "axes[2].set_ylabel(\"Learning Rate\")\n",
    "axes[2].set_title(\"Learning Rate Schedule\")\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"training_curves.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Final train loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"  Final val loss: {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"  Best val loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation & Inference\n",
    "\n",
    "Generate samples with your fine-tuned model and compare to the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Test prompts - rap style!\n",
    "TEST_PROMPTS = [\n",
    "    \"[S1] Yeah, I'm spitting fire on the mic tonight, gonna show them what I got, rising to the top!\",\n",
    "    \"[S1] The rhythm flows through me like water, every beat hits harder, I'm a natural born starter!\",\n",
    "    \"[S1] Check it out, I'm the one they've been waiting for, coming through the door, ready to explore!\",\n",
    "    \"[S1] Money on my mind, grind never stops, from the bottom to the top, watch me drop!\",\n",
    "    \"[S1] Real recognize real, that's the deal, keep it trill, got the skill to make you feel!\",\n",
    "]\n",
    "\n",
    "# Use a random training file as speaker reference\n",
    "SPEAKER_AUDIO_PATH = random.choice(audio_files) if audio_files else None\n",
    "\n",
    "print(f\"Using speaker reference: {SPEAKER_AUDIO_PATH}\")\n",
    "print(f\"\\nWill generate {len(TEST_PROMPTS)} samples...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples with fine-tuned model\n",
    "@torch.inference_mode()\n",
    "def generate_audio(model, text, speaker_audio_path=None, seed=0):\n",
    "    \"\"\"Generate audio using the (fine-tuned) model.\"\"\"\n",
    "    \n",
    "    # Load speaker audio\n",
    "    if speaker_audio_path:\n",
    "        speaker_audio = load_audio(speaker_audio_path)\n",
    "    else:\n",
    "        speaker_audio = None\n",
    "    \n",
    "    # Create sample function\n",
    "    sample_fn = partial(\n",
    "        sample_euler_cfg_independent_guidances,\n",
    "        num_steps=40,\n",
    "        cfg_scale_text=3.0,\n",
    "        cfg_scale_speaker=8.0,\n",
    "        cfg_min_t=0.5,\n",
    "        cfg_max_t=1.0,\n",
    "        truncation_factor=0.8,\n",
    "        rescale_k=None,\n",
    "        rescale_sigma=None,\n",
    "        speaker_kv_scale=None,\n",
    "        speaker_kv_max_layers=None,\n",
    "        speaker_kv_min_t=None,\n",
    "        sequence_length=640,\n",
    "    )\n",
    "    \n",
    "    # Generate\n",
    "    audio_out, normalized_text = sample_pipeline(\n",
    "        model=model,\n",
    "        fish_ae=fish_ae,\n",
    "        pca_state=pca_state,\n",
    "        sample_fn=sample_fn,\n",
    "        text_prompt=text,\n",
    "        speaker_audio=speaker_audio,\n",
    "        rng_seed=seed,\n",
    "    )\n",
    "    \n",
    "    return audio_out[0].cpu(), normalized_text\n",
    "\n",
    "# Generate and play samples\n",
    "print(\"Generating samples with fine-tuned model...\\n\")\n",
    "\n",
    "for i, prompt in enumerate(TEST_PROMPTS):\n",
    "    print(f\"Prompt {i + 1}: {prompt}\")\n",
    "    \n",
    "    audio, _ = generate_audio(\n",
    "        model,\n",
    "        prompt,\n",
    "        speaker_audio_path=str(SPEAKER_AUDIO_PATH) if SPEAKER_AUDIO_PATH else None,\n",
    "        seed=i,\n",
    "    )\n",
    "    \n",
    "    # Save audio\n",
    "    output_path = os.path.join(OUTPUT_DIR, f\"sample_{i + 1}.wav\")\n",
    "    torchaudio.save(output_path, audio.unsqueeze(0), 44100)\n",
    "    print(f\"Saved to: {output_path}\")\n",
    "    \n",
    "    # Play audio\n",
    "    display(Audio(audio.numpy(), rate=44100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Checkpoint for Later Use\n",
    "\n",
    "Use this section to load a saved LoRA checkpoint onto a fresh model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load a saved LoRA checkpoint\n",
    "# Uncomment and run this cell to load a checkpoint\n",
    "\n",
    "# CHECKPOINT_PATH = os.path.join(OUTPUT_DIR, \"lora_best.pt\")\n",
    "# \n",
    "# # Load fresh base model\n",
    "# model_fresh = load_model_from_hf(\n",
    "#     device=DEVICE,\n",
    "#     dtype=DTYPE,\n",
    "#     compile=False,\n",
    "#     delete_blockwise_modules=True,\n",
    "# )\n",
    "# \n",
    "# # Load checkpoint to get config\n",
    "# checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "# config = checkpoint[\"config\"]\n",
    "# \n",
    "# # Apply LoRA with saved config\n",
    "# model_fresh, _ = apply_lora_to_model(\n",
    "#     model_fresh,\n",
    "#     rank=config[\"rank\"],\n",
    "#     alpha=config[\"alpha\"],\n",
    "#     dropout=0.0,  # No dropout for inference\n",
    "#     target_modules=config[\"target_modules\"],\n",
    "# )\n",
    "# \n",
    "# # Load LoRA weights\n",
    "# load_lora_checkpoint(model_fresh, CHECKPOINT_PATH, device=DEVICE)\n",
    "# model_fresh.eval()\n",
    "# \n",
    "# print(f\"Loaded checkpoint from epoch {config['epoch']} (loss: {config['loss']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Tips & Notes for Rap Training\n",
    "\n",
    "### What We Configured for 225 Samples\n",
    "- **LoRA rank 32**: Higher rank captures more style nuance with larger dataset\n",
    "- **Lower dropout (0.05)**: Less regularization needed with more data\n",
    "- **Higher learning rate (1e-4)**: Can train faster with more data\n",
    "- **Fewer epochs (10)**: More data means fewer passes needed\n",
    "- **Whisper medium**: Better accuracy for rap lyrics than base model\n",
    "\n",
    "### Training Expectations\n",
    "With 225 rap acapellas:\n",
    "- **Training time**: ~2-4 hours on T4, ~1-2 hours on A100\n",
    "- **Expected final loss**: ~0.05-0.15 (lower is better)\n",
    "- **Checkpoint size**: ~80-100MB\n",
    "\n",
    "### If Results Sound Off\n",
    "1. **Too monotone**: Increase `cfg_scale_text` to 4-5 during inference\n",
    "2. **Wrong rhythm**: The model learned general rap style, not specific flows\n",
    "3. **Voice doesn't match**: Try different speaker reference audio\n",
    "4. **Gibberish output**: Check if transcriptions were accurate\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "**Out of Memory (OOM)**:\n",
    "- Reduce `MAX_LATENT_LENGTH` to 320 (15 seconds max)\n",
    "- Use A100 GPU on Colab instead of T4\n",
    "- Reduce `LORA_RANK` to 16\n",
    "\n",
    "**Loss not decreasing**:\n",
    "- Check transcriptions are accurate (rap lyrics are hard!)\n",
    "- Try `WHISPER_MODEL = \"large-v3\"` for better transcription\n",
    "\n",
    "**Validation loss increasing (overfitting)**:\n",
    "- Increase `LORA_DROPOUT` to 0.1\n",
    "- Reduce `NUM_EPOCHS`\n",
    "- Reduce `LORA_RANK` to 16\n",
    "\n",
    "### Voice Cloning Still Works!\n",
    "The speaker path (wk_speaker, wv_speaker) was kept frozen, so you can:\n",
    "- Use ANY speaker reference audio at inference time\n",
    "- The rap style transfers to any voice\n",
    "- Original voice cloning quality is preserved"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
