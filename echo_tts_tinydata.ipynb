{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Echo-TTS Rap Style Fine-Tuning\n",
    "\n",
    "This notebook implements LoRA-based fine-tuning for Echo-TTS to adapt the model to **rapping style** while preserving voice cloning capabilities.\n",
    "\n",
    "**Dataset:**\n",
    "- 225 rap acapellas (unprocessed)\n",
    "- Requires preprocessing: segmentation, transcription\n",
    "\n",
    "**Requirements:**\n",
    "- GPU with 16GB+ VRAM (T4/A100 on Colab)\n",
    "\n",
    "**What you'll get:**\n",
    "- Fine-tuned model that generates rap-style speech\n",
    "- LoRA checkpoint (~50-100MB) that can be loaded on top of base model\n",
    "- Preserved voice cloning - can still use any speaker reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo already exists\n",
      "Working directory: /content/echo-tts\n"
     ]
    }
   ],
   "source": [
    "# Clone the echo-tts repository\n",
    "!git clone https://github.com/CoreBedtime/echo-tts.git 2>/dev/null || echo \"Repo already exists\"\n",
    "\n",
    "# Add to Python path\n",
    "import sys\n",
    "sys.path.insert(0, '/content/echo-tts')\n",
    "\n",
    "# Change working directory\n",
    "import os\n",
    "os.chdir('/content/echo-tts')\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q torch torchaudio safetensors huggingface-hub einops\n",
    "!pip install -q openai-whisper  # For automatic transcription\n",
    "!pip install -q torchcodec  # For audio decoding\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.1)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (0.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchaudio safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for saving checkpoints (Colab)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100-SXM4-40GB\n",
      "VRAM: 42.5 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio directory: /content/dataset/\n",
      "Output directory: ./checkpoints/\n",
      "Device: cuda\n",
      "\n",
      "Training config for 225 samples:\n",
      "  LoRA rank: 32\n",
      "  Learning rate: 0.0001\n",
      "  Epochs: 10\n",
      "  Effective batch size: 8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# =============================================================================\n",
    "# PATHS - Update these!\n",
    "# =============================================================================\n",
    "\n",
    "# Directory containing your training audio files\n",
    "AUDIO_DIR = \"/content/dataset/\"  # Put your rap acapellas here\n",
    "\n",
    "# Where to save checkpoints\n",
    "OUTPUT_DIR = \"./checkpoints/\" \n",
    "\n",
    "# =============================================================================\n",
    "# LoRA CONFIGURATION (optimized for 225 samples)\n",
    "# =============================================================================\n",
    "\n",
    "LORA_RANK = 32          # Higher rank for larger dataset (more expressiveness)\n",
    "LORA_ALPHA = 32.0       # Scaling factor, typically equal to rank\n",
    "LORA_DROPOUT = 0.05     # Lower dropout - more data means less regularization needed\n",
    "\n",
    "# Which modules to train (default preserves voice cloning path)\n",
    "# Speaker path (wk_speaker, wv_speaker) is NOT trained to preserve cloning\n",
    "TARGET_MODULES = [\n",
    "    # Main decoder attention (style)\n",
    "    \"blocks.*.attention.wq\",\n",
    "    \"blocks.*.attention.wk\",\n",
    "    \"blocks.*.attention.wv\",\n",
    "    \"blocks.*.attention.wo\",\n",
    "    # Text cross-attention (text-to-audio mapping)\n",
    "    \"blocks.*.attention.wk_text\",\n",
    "    \"blocks.*.attention.wv_text\",\n",
    "    # MLP layers (feature transformation)\n",
    "    \"blocks.*.mlp.w1\",\n",
    "    \"blocks.*.mlp.w2\",\n",
    "    \"blocks.*.mlp.w3\",\n",
    "]\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING CONFIGURATION (optimized for 225 samples)\n",
    "# =============================================================================\n",
    "\n",
    "LEARNING_RATE = 1e-4     # Slightly higher LR for larger dataset\n",
    "NUM_EPOCHS = 10          # Fewer epochs needed with more data\n",
    "BATCH_SIZE = 1           # Batch size (1 for memory efficiency)\n",
    "GRADIENT_ACCUMULATION = 8  # Larger effective batch for stability\n",
    "MAX_GRAD_NORM = 1.0      # Gradient clipping\n",
    "WARMUP_STEPS = 100       # More warmup steps for larger dataset\n",
    "\n",
    "# Audio settings\n",
    "MAX_LATENT_LENGTH = 640  # Max ~30 seconds (reduce to 320 if OOM)\n",
    "SEGMENT_DURATION = 25.0  # Split long audio into ~25 second chunks\n",
    "MIN_SEGMENT_DURATION = 5.0  # Minimum segment length to keep\n",
    "\n",
    "# Whisper model for transcription (use larger model for rap lyrics accuracy)\n",
    "WHISPER_MODEL = \"medium\"  # Options: tiny, base, small, medium, large-v3\n",
    "\n",
    "# Device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.bfloat16   # Use bfloat16 for training\n",
    "\n",
    "# Validation split\n",
    "VAL_SPLIT = 0.1  # 5% for validation (~11 samples)\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(AUDIO_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Audio directory: {AUDIO_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"\\nTraining config for 225 samples:\")\n",
    "print(f\"  LoRA rank: {LORA_RANK}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading EchoDiT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: 2800.7M parameters\n",
      "\n",
      "Loading Fish-S1-DAC autoencoder...\n",
      "Autoencoder loaded\n",
      "\n",
      "Loading PCA state...\n",
      "PCA state loaded\n",
      "\n",
      "==================================================\n",
      "All models loaded successfully!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from inference import (\n",
    "    load_model_from_hf,\n",
    "    load_fish_ae_from_hf,\n",
    "    load_pca_state_from_hf,\n",
    "    ae_decode,\n",
    "    get_speaker_latent_and_mask,\n",
    "    get_text_input_ids_and_mask,\n",
    "    sample_euler_cfg_independent_guidances,\n",
    "    sample_pipeline,\n",
    "    load_audio,\n",
    ")\n",
    "from functools import partial\n",
    "\n",
    "print(\"Loading EchoDiT model...\")\n",
    "model = load_model_from_hf(\n",
    "    device=DEVICE,\n",
    "    dtype=DTYPE,\n",
    "    compile=False,  # Don't compile for training\n",
    "    delete_blockwise_modules=True,  # Save memory\n",
    ")\n",
    "print(f\"Model loaded: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M parameters\")\n",
    "\n",
    "print(\"\\nLoading Fish-S1-DAC autoencoder...\")\n",
    "fish_ae = load_fish_ae_from_hf(\n",
    "    device=DEVICE,\n",
    "    dtype=torch.float32,  # AE needs float32 for quality\n",
    ")\n",
    "print(\"Autoencoder loaded\")\n",
    "\n",
    "print(\"\\nLoading PCA state...\")\n",
    "pca_state = load_pca_state_from_hf(device=DEVICE)\n",
    "print(\"PCA state loaded\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"All models loaded successfully!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation\n",
    "\n",
    "With 225 rap acapellas, we need to:\n",
    "1. List and validate all audio files\n",
    "2. Segment long tracks into training-sized chunks (~25 seconds)\n",
    "3. Transcribe using Whisper (medium model for better rap lyrics)\n",
    "4. Create train/validation split\n",
    "5. Pre-encode all audio to latents (cached for fast training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 225 audio files\n",
      "==================================================\n",
      "\n",
      "Dataset Statistics:\n",
      "  Total files: 225\n",
      "  Total duration: 43595.4s (726.6 min, 12.11 hours)\n",
      "  Average duration: 193.8s\n",
      "  Shortest: 59.4s\n",
      "  Longest: 415.4s\n",
      "\n",
      "Duration distribution:\n",
      "  30-60s: 1 files\n",
      "  60-120s: 28 files\n",
      "  120-180s: 63 files\n",
      "  180-300s: 122 files\n",
      "  >300s: 11 files\n"
     ]
    }
   ],
   "source": [
    "# List and analyze all audio files\n",
    "from pathlib import Path\n",
    "import random\n",
    "import torchaudio\n",
    "import torchcodec\n",
    "\n",
    "AUDIO_EXTENSIONS = (\".mp3\", \".wav\", \".flac\", \".ogg\", \".m4a\")\n",
    "\n",
    "audio_dir = Path(AUDIO_DIR)\n",
    "audio_files = []\n",
    "for ext in AUDIO_EXTENSIONS:\n",
    "    audio_files.extend(audio_dir.glob(f\"*{ext}\"))\n",
    "    audio_files.extend(audio_dir.glob(f\"*{ext.upper()}\"))\n",
    "    # Also check subdirectories\n",
    "    audio_files.extend(audio_dir.glob(f\"**/*{ext}\"))\n",
    "    audio_files.extend(audio_dir.glob(f\"**/*{ext.upper()}\"))\n",
    "\n",
    "# Remove duplicates and sort\n",
    "audio_files = sorted(set(audio_files))\n",
    "\n",
    "print(f\"Found {len(audio_files)} audio files\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Analyze durations\n",
    "durations = []\n",
    "valid_files = []\n",
    "for f in audio_files:\n",
    "    try:\n",
    "        waveform, sample_rate = torchaudio.load(str(f))\n",
    "        duration = waveform.shape[1] / sample_rate\n",
    "        durations.append(duration)\n",
    "        valid_files.append(f)\n",
    "    except Exception as e:\n",
    "        print(f\"  Skipping {f.name}: {e}\")\n",
    "\n",
    "audio_files = valid_files\n",
    "total_duration = sum(durations)\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Total files: {len(audio_files)}\")\n",
    "print(f\"  Total duration: {total_duration:.1f}s ({total_duration/60:.1f} min, {total_duration/3600:.2f} hours)\")\n",
    "print(f\"  Average duration: {sum(durations)/len(durations):.1f}s\")\n",
    "print(f\"  Shortest: {min(durations):.1f}s\")\n",
    "print(f\"  Longest: {max(durations):.1f}s\")\n",
    "\n",
    "# Show duration distribution\n",
    "print(f\"\\nDuration distribution:\")\n",
    "for bucket in [(0, 30), (30, 60), (60, 120), (120, 180), (180, 300), (300, float('inf'))]:\n",
    "    count = sum(1 for d in durations if bucket[0] <= d < bucket[1])\n",
    "    if count > 0:\n",
    "        label = f\"{bucket[0]}-{bucket[1]}s\" if bucket[1] != float('inf') else f\">{bucket[0]}s\"\n",
    "        print(f\"  {label}: {count} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transcribing 225 files with Whisper (medium)...\n",
      "This may take 30-60 minutes for 225 files...\n",
      "Progress is saved - you can interrupt and resume.\n",
      "\n",
      "Loading Whisper model 'medium'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 1.42G/1.42G [00:04<00:00, 353MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing 1/10: 001_Nas - Life's A Bitch (Acapella).mp3\n",
      "Transcribing 2/10: 001_Roc Marciano - Whateva Whateva Acapella 86 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 3/10: 002_Roc Marciano - Went Diamond Acapella 77 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 4/10: 003_Roc Marciano - Travel Fox Acapella 75 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 5/10: 004_Roc Marciano - RocMarkable Acapella 89 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 6/10: 005_Roc Marciano - Ridin' Around Acapella 87 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 7/10: 006_Roc Marciano - Prayer Hands Acapella 88BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 8/10: 007_Roc Marciano - LeFlair Acapella 80 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 9/10: 008_2Pac feat. Scarface - Smile (Acapella).mp3\n",
      "Transcribing 10/10: 008_Roc Marciano - Don Shit Acapella 88 BPM - 4K Acapella Cat.mp3\n",
      "Progress: 10/225 transcribed\n",
      "Loading Whisper model 'medium'...\n",
      "Transcribing 1/10: 009_Roc Marciano - BeBe's Kids Acapella 68 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 2/10: 010_Cuban Link Ft. Big Pun - Toe to Toe (Acapella).mp3\n",
      "Transcribing 3/10: 010_Earl Sweatshirt, Maxo - WHOLE WORLD Acapella 71 BPM - 4K Acapella cat.mp3\n",
      "Transcribing 4/10: 011_Earl Sweatshirt - Lye Acapella 75 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 5/10: 012_Earl Sweatshirt - 2010 Acapella 106 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 6/10: 012_Redman - Can't Wait (Acapella).mp3\n",
      "Transcribing 7/10: 013_Earl Sweatshirt, The Alchemist - Sirius Blac Acapella 53 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 8/10: 014_Earl Sweatshirt, The Alchemist - 27 Braids Acapella 81 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 9/10: 015_Ear Sweatshirt, The Alchemist - Heat Check Acapella 72 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 10/10: 015_Nas-Made You Look(Acapella).mp3\n",
      "Progress: 20/225 transcribed\n",
      "Loading Whisper model 'medium'...\n",
      "Transcribing 1/10: 016_AZ - Doe or Die (Acapella).mp3\n",
      "Transcribing 2/10: 016_Earl Sweatshirt, The Alchemist - 100 High Street Acapella 82 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 3/10: 017_Dead Prez - Hip Hop (Acapella Version).mp3\n",
      "Transcribing 4/10: 017_Earl Sweatshirt - Static Acapella 70 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 5/10: 018_Earl Sweatshirt - Gamma Acapella 74 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 6/10: 019_Freddie Gibbs, The Alchemist - Mar-a-Lago Acapella 70 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 7/10: 020_Freddie Gibbs - Blackest in the Room Acapella 79 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 8/10: 021_Busta Rhymes - Woo Hah!! (Acapella).mp3\n",
      "Transcribing 9/10: 021_Freddie Gibbs, The Alchemist - A Thousand Mountains Acapella 75 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 10/10: 022_Freddie Gibbs, The Alchemist - Skinny Suge II Acapella 79 BPM - 4K Acapella Cat.mp3\n",
      "Progress: 30/225 transcribed\n",
      "Loading Whisper model 'medium'...\n",
      "Transcribing 1/10: 022_Kool G Rap ft Nas Fast Life Acapella.mp3\n",
      "Transcribing 2/10: 024_Freddie Gibbs, Madlib - Half Manne Half Cocaine Acapella 130 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 3/10: 025_A Tribe Called Quest - 1nce Again (Acapella).mp3\n",
      "Transcribing 4/10: 025_Freddie Gibbs, Madlib - Real Acapella 98 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 5/10: 026_Biggie Smalls-Machine Gun Funk (Acapella).mp3\n",
      "Transcribing 6/10: 026_Freddie Gibbs, Madlib, Raekwon - Bomb Acapella 91 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 7/10: 027_Freddie Gibbs, Madlib - Scarface Acapella 91 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 8/10: 027_Tupac and Biggie- Runnin (Acapella).mp3\n",
      "Transcribing 9/10: 028_2Pac - N.I.G.G.A.  (OG) (Studio Acapella).mp3\n",
      "Transcribing 10/10: 028_Freddie Gibbs, The Alchemist - Skinny Suge Acapella 75 BPM - 4K Acapella Cat.mp3\n",
      "Progress: 40/225 transcribed\n",
      "Loading Whisper model 'medium'...\n",
      "Transcribing 1/10: 029_Mach-Hommy, Tha God Fahim - Cold Milk Acapella 87 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 2/10: 029_Nas -Ether Acapella.mp3\n",
      "Transcribing 3/10: 030_Mach-Hommy, DJ Muggs, Tha God Fahim - Kouign-Amman Acapella 95 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 4/10: 031_Biz Markie - Just A Friend (Acapella).mp3\n",
      "Transcribing 5/10: 031_Mach-Hommy, DJ Muggs - 900K Acapella 81 BPM -4K Acapella Cat.mp3\n",
      "Transcribing 6/10: 032_Mach-Hommy, Your Old Droog - Pour House Acapella 90 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 7/10: 032_Nas & AZ - Life's A Bitch (Acapella).mp3\n",
      "Transcribing 8/10: 033_Gravediggaz - 1-800 Suicide (Acapella).mp3\n",
      "Transcribing 9/10: 033_Mach-Hommy, Earl Sweatshirt - Soon Jah Due Acapella 70 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 10/10: 034_Mach-Hommy - Regina Acapella 93 BPM - 4K Acapella Cat.mp3\n",
      "Progress: 50/225 transcribed\n",
      "Loading Whisper model 'medium'...\n",
      "Transcribing 1/10: 034_Nas - Nas Is Like (Acapella).mp3\n",
      "Transcribing 2/10: 035_INI & Pete Rock - Fakin' Jax (Acapella).mp3\n",
      "Transcribing 3/10: 035_Mach-Hommy - Honey Roasted Acapella 85 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 4/10: 036_Mach-Hommy - Ten Boxes - Sin Eater Acapella 85 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 5/10: 037_Mach-Hommy - Blockchain Acapella 87 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 6/10: 038_Boldy James, Conductor Williams - The Ol Switcharoo Acapella 60 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 7/10: 038_Notorious BIG Feat. Wu-Tang Clan - 3 Bricks (Acapella).mp3\n",
      "Transcribing 8/10: 039_Boldy James, RichGains - Skinny Me Acapella 72 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 9/10: 040_Boldy James, RichGains - Janky Acapella 85 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 10/10: 041_2Pac - Changes (Acapella) [HD].mp3\n",
      "Progress: 60/225 transcribed\n",
      "Loading Whisper model 'medium'...\n",
      "Transcribing 1/10: 041_Boldy James, Curren$y & Freddie Gibbs - Fake Flowers Acapella 80 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 2/10: 042_Boldy James, The Alchemist - E.P.M.D Acapella 71 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 3/10: 043_Boldy James, The Alchemist - Mustard Acapella 52 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 4/10: 043_Xzibit, Ras Kass & Saafir - 3 Card Molly (Acapella).mp3\n",
      "Transcribing 5/10: 044_Boldy James, The Alchemist & Vince Staples - Surf & Turf Acapella 83 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 6/10: 045_Boldy James, Nicholas Craven - Nice Try Wrong Guy Acapella 75.5 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 7/10: 046_Boldy James, The Alchemist & Evidence - Grey October Acapella 70 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 8/10: 047_MF DOOM - Figaro Acapella 92 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 9/10: 047_Ol' Dirty Bastard - Dog Shit.mp3\n",
      "Transcribing 10/10: 048_Gza - Labels (acapella).mp3\n",
      "Progress: 70/225 transcribed\n",
      "Loading Whisper model 'medium'...\n",
      "Transcribing 1/10: 048_MF DOOM - THAT'S THAT - Acapella 95 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 2/10: 049_Madvillain , Madlib MF DOOM - Accordion Acapella 96 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 3/10: 049_ODB Interview.mp3\n",
      "Transcribing 4/10: 050_MF DOOM feat. Stahhr, 4ize - Guinnessez Acapella 83 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 5/10: 050_OL'DIRTY - FREESTYLE RARE FOOTAGE(R.I.P ODB).mp3\n",
      "Transcribing 6/10: 051_MF DOOM - Kon Karne Acapella 92 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 7/10: 052_MF DOOM feat. Pebbles The Invisible Girl - The Mic Acapella 96 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 8/10: 053_MF DOOM feat MF Grimm - Tick, Tick... Acapella 92-60 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 9/10: 054_MF DOOM - Go With the Flow Acapella 94 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 10/10: 055_Benny the Butcher - How To Rap Acapella 84 BPM - 4K Acapella Cat.mp3\n",
      "Progress: 80/225 transcribed\n",
      "Loading Whisper model 'medium'...\n",
      "Transcribing 1/10: 056_Benny the Butcher feat. Lil Wayne - Big Dog Acapella 74 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 2/10: 057_Benny the Butcher - Billy Joe Acapella 73 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 3/10: 058_Benny the Butcher - Super Plug Acapella 81 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 4/10: 059_Benny the Butcher feat. Stove God Cooks - One Foot In Acapella 83 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 5/10: 060_Benny the Butcher - Fast Eddie Acapella 83 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 6/10: 061_Benny the Butcher - Echo Long Acapella 73 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 7/10: 062_Benny the Butcher - Broken Bottles Acapella 72 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 8/10: 063_Benny the Butcher feat. Styles P - Toxic Acapella 82 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 9/10: 064_Benny the Butcher  feat. Boldy James - Duffel Bag Hottie's Revenge Acapella 85 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 10/10: 065_Conway the Machine feat. Mach-Hommy, Benny the Butcher - Beloved Acapella 76 BPM - 4K Acapella Cat.mp3\n",
      "Progress: 90/225 transcribed\n",
      "Loading Whisper model 'medium'...\n",
      "Transcribing 1/10: 066_Conway the Machine feat. Westside Gunn - Air Holez Acapella 70 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 2/10: 067_Conway the Machine - Nothin Less Acapella 87 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 3/10: 068_Conway the Machine - Brick Fare Acapella 77 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 4/10: 069_Conway the Machine - Monogram Acapella 79 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 5/10: 070_Big Ghost Ltd, Conway the Machine, Method Man - Scared II Death Acapella 80 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 6/10: 071_Big Ghost Ltd, Conway the Machine, Jae Skeese - In My Soul Acapella 77 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 7/10: 072_Conway the Machine with Wallo267 - Stressed Acapella 77 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 8/10: 073_Conway the Machine - Piano Love Acapella 76 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 9/10: 074_Rome Streetz - Wheel Of Fortune Acapella 85 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 10/10: 075_Rome Streetz - Ace Of Swords Acapella 82 BPM - 4K Acapella Cat.mp3\n",
      "Progress: 100/225 transcribed\n",
      "Loading Whisper model 'medium'...\n",
      "Transcribing 1/10: 076_Rome Streetz - Hell Backwards Acapella 79 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 2/10: 077_Rome Streetz feat. Cormega - Weight Of The World Acapella 75 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 3/10: 078_Rome Streetz - Starbvxkz Acapella 84 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 4/10: 079_Rome Streetz - Procall Acapella 74 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 5/10: 080_Rome Streetz - Rich Porter or Pookie Acapella 78 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 6/10: 081_Rome Streetz - In Too Deep Acapella 85 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 7/10: 082_Rome Streets - Big Steppa Acapella 82 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 8/10: 083_Wu Tang Clan feat. Inspectah Deck & Nas - Let My Niggas Live Acapella 94 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 9/10: 084_Wu Tang Clan - Tearz Acapella 92 BPM - 4K Acapella Cât.mp3\n",
      "Transcribing 10/10: 085_Wu Tang Clan - Wu Tang： 7th Chamber Acapella 92 BPM - 4K Acapella Cat.mp3\n",
      "Progress: 110/225 transcribed\n",
      "Loading Whisper model 'medium'...\n",
      "Transcribing 1/10: 086_Wu Tang Clan - Method Man Acapella 102 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 2/10: 087_Wu Tang Clan - Wu Tang Clan Ain't Nuthing ta F' Wit Acapella 99 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 3/10: 088_Wu Tang Clan - Da Mystery of Chessboxin' Acapella 105 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 4/10: 089_Wu Tang Clan - Shame On a Nigga Acapella 100 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 5/10: 090_Wu Tang Clan - Clan In Da Front Acapella 96 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 6/10: 091_Wu Tang Clan - C.R.E.A.M Acapella 91 BPM - 4K Acapella Cât.mp3\n",
      "Transcribing 7/10: 092_Aesop Rock - Checkers Acapella 84 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 8/10: 093_Westside Gunn feat. Mach Hommy & DJ Qbert - King City Acapella 94 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 9/10: 094_Westside Gunn - Chine Gun Acapella 71 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 10/10: 095_Westside Gunn feat. Danny Brown - Bodies on Fairfax Acapella 64 BPM - 4K Acapella Cat.mp3\n",
      "Progress: 120/225 transcribed\n",
      "Loading Whisper model 'medium'...\n",
      "Transcribing 1/10: 096_Westside Gunn feat. Mayhem Lauren - Over Gold Acapella 74 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 2/10: 097_Westside Gunn feat. Conway The Machine - Dunks Acapella 78 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 3/10: 098_Westside Gunn feat. Conway The Machine - Free Chapo Acapella 74 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 4/10: 099_Westside Gunn feat. Your Old Droog - Vivian at the Art Basel Acapella 73 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 5/10: 100_Westside Gunn feat. Benny The Butcher - Shower Shoe Lords 85 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 6/10: 101_Westside Gunn feat. Keisha Plum - Gustavo Acapella 75 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 7/10: 102_Snoop Dogg, D.O.C, RBX, Tha Dogg Pound - Serial Killa Acapella 99 BPM - 4K Acapella Cat.mp3\n",
      "Transcribing 8/10: 103_Nice & Smooth - Funky For You Acapella 100 BPM - 4K Acapella Cat.mp3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Transcribe all audio files using Whisper\n",
    "# This will take a while for 225 files - save progress as we go\n",
    "\n",
    "from train_utils import transcribe_audio_files\n",
    "import json\n",
    "\n",
    "TRANSCRIPTION_CACHE = os.path.join(OUTPUT_DIR, \"transcriptions.json\")\n",
    "\n",
    "# Check if we have cached transcriptions\n",
    "if os.path.exists(TRANSCRIPTION_CACHE):\n",
    "    print(f\"Loading cached transcriptions from {TRANSCRIPTION_CACHE}\")\n",
    "    with open(TRANSCRIPTION_CACHE, \"r\") as f:\n",
    "        transcriptions = json.load(f)\n",
    "    print(f\"Loaded {len(transcriptions)} cached transcriptions\")\n",
    "    \n",
    "    # Find files that still need transcription\n",
    "    cached_paths = set(transcriptions.keys())\n",
    "    files_to_transcribe = [f for f in audio_files if str(f) not in cached_paths]\n",
    "    print(f\"Files still needing transcription: {len(files_to_transcribe)}\")\n",
    "else:\n",
    "    transcriptions = {}\n",
    "    files_to_transcribe = audio_files\n",
    "\n",
    "if len(files_to_transcribe) > 0:\n",
    "    print(f\"\\nTranscribing {len(files_to_transcribe)} files with Whisper ({WHISPER_MODEL})...\")\n",
    "    print(\"This may take 30-60 minutes for 225 files...\")\n",
    "    print(\"Progress is saved - you can interrupt and resume.\\n\")\n",
    "    \n",
    "    # Transcribe in batches and save progress\n",
    "    BATCH_SIZE_TRANSCRIBE = 10\n",
    "    \n",
    "    for batch_start in range(0, len(files_to_transcribe), BATCH_SIZE_TRANSCRIBE):\n",
    "        batch_files = files_to_transcribe[batch_start:batch_start + BATCH_SIZE_TRANSCRIBE]\n",
    "        \n",
    "        batch_transcriptions = transcribe_audio_files(\n",
    "            audio_paths=[str(f) for f in batch_files],\n",
    "            model_name=WHISPER_MODEL,\n",
    "            language=\"en\",\n",
    "        )\n",
    "        \n",
    "        transcriptions.update(batch_transcriptions)\n",
    "        \n",
    "        # Save progress\n",
    "        with open(TRANSCRIPTION_CACHE, \"w\") as f:\n",
    "            json.dump(transcriptions, f, indent=2)\n",
    "        \n",
    "        print(f\"Progress: {min(batch_start + BATCH_SIZE_TRANSCRIBE, len(files_to_transcribe))}/{len(files_to_transcribe)} transcribed\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Transcription complete: {len(transcriptions)} files\")\n",
    "print(f\"Saved to: {TRANSCRIPTION_CACHE}\")\n",
    "\n",
    "# Show a few samples\n",
    "print(f\"\\nSample transcriptions:\")\n",
    "for i, (path, text) in enumerate(list(transcriptions.items())[:3]):\n",
    "    filename = Path(path).name\n",
    "    print(f\"\\n{filename}:\")\n",
    "    print(f\"  {text[:150]}{'...' if len(text) > 150 else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Edit transcriptions manually if Whisper made mistakes\n",
    "# Uncomment and modify as needed:\n",
    "\n",
    "# transcriptions[\"/path/to/file.mp3\"] = \"[S1] Your corrected transcription here.\"\n",
    "\n",
    "# Tips for transcriptions:\n",
    "# - Start with [S1] for single speaker\n",
    "# - Use commas for pauses\n",
    "# - Exclamation marks increase expressiveness\n",
    "# - Keep punctuation natural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset with train/val split\n",
    "from train_utils import (\n",
    "    TrainingSample,\n",
    "    EchoTTSDataset,\n",
    "    collate_fn,\n",
    "    segment_audio,\n",
    "    load_audio_tensor,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "# Create training samples\n",
    "all_samples = []\n",
    "for path, text in transcriptions.items():\n",
    "    if text and len(text.strip()) > 10:  # Filter out empty/very short transcriptions\n",
    "        all_samples.append(TrainingSample(\n",
    "            audio_path=path,\n",
    "            text=text,\n",
    "            speaker_audio_path=None,  # Use same audio as speaker reference\n",
    "        ))\n",
    "\n",
    "print(f\"Created {len(all_samples)} training samples (filtered {len(transcriptions) - len(all_samples)} empty)\")\n",
    "\n",
    "# Shuffle and split into train/val\n",
    "random.seed(42)\n",
    "random.shuffle(all_samples)\n",
    "\n",
    "val_size = max(1, int(len(all_samples) * VAL_SPLIT))\n",
    "train_samples = all_samples[val_size:]\n",
    "val_samples = all_samples[:val_size]\n",
    "\n",
    "print(f\"Train samples: {len(train_samples)}\")\n",
    "print(f\"Validation samples: {len(val_samples)}\")\n",
    "\n",
    "# Create datasets\n",
    "print(\"\\nCreating training dataset and encoding audio to latents...\")\n",
    "print(\"This will take a few minutes for 225 files...\\n\")\n",
    "\n",
    "train_dataset = EchoTTSDataset(\n",
    "    samples=train_samples,\n",
    "    fish_ae=fish_ae,\n",
    "    pca_state=pca_state,\n",
    "    device=DEVICE,\n",
    "    max_latent_length=MAX_LATENT_LENGTH,\n",
    "    cache_latents=True,\n",
    ")\n",
    "\n",
    "print(\"\\nCreating validation dataset...\")\n",
    "val_dataset = EchoTTSDataset(\n",
    "    samples=val_samples,\n",
    "    fish_ae=fish_ae,\n",
    "    pca_state=pca_state,\n",
    "    device=DEVICE,\n",
    "    max_latent_length=MAX_LATENT_LENGTH,\n",
    "    cache_latents=True,\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Dataset ready!\")\n",
    "print(f\"  Train: {len(train_dataset)} samples, {len(train_dataloader)} batches/epoch\")\n",
    "print(f\"  Val: {len(val_dataset)} samples, {len(val_dataloader)} batches\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply LoRA to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lora import (\n",
    "    apply_lora_to_model,\n",
    "    count_parameters,\n",
    "    get_lora_params,\n",
    "    save_lora_checkpoint,\n",
    "    load_lora_checkpoint,\n",
    ")\n",
    "\n",
    "# Apply LoRA adapters to the model\n",
    "print(\"Applying LoRA adapters...\")\n",
    "print(f\"  Rank: {LORA_RANK}\")\n",
    "print(f\"  Alpha: {LORA_ALPHA}\")\n",
    "print(f\"  Dropout: {LORA_DROPOUT}\")\n",
    "print(f\"  Target modules: {len(TARGET_MODULES)} patterns\")\n",
    "\n",
    "model, lora_modules = apply_lora_to_model(\n",
    "    model,\n",
    "    rank=LORA_RANK,\n",
    "    alpha=LORA_ALPHA,\n",
    "    dropout=LORA_DROPOUT,\n",
    "    target_modules=TARGET_MODULES,\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "print(f\"\\nParameter counts:\")\n",
    "print(f\"  Total: {total_params / 1e6:.1f}M\")\n",
    "print(f\"  Trainable (LoRA): {trainable_params / 1e6:.2f}M ({100 * trainable_params / total_params:.2f}%)\")\n",
    "print(f\"  LoRA modules applied: {len(lora_modules)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from train_utils import train_epoch, get_cosine_schedule_with_warmup, training_step\n\n# Setup optimizer (only LoRA params)\nlora_params = get_lora_params(model)\noptimizer = torch.optim.AdamW(\n    lora_params,\n    lr=LEARNING_RATE,\n    weight_decay=0.01,\n    betas=(0.9, 0.999),\n)\n\n# Learning rate scheduler\nnum_training_steps = len(train_dataloader) * NUM_EPOCHS // GRADIENT_ACCUMULATION\nscheduler = get_cosine_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=min(WARMUP_STEPS, num_training_steps // 10),\n    num_training_steps=num_training_steps,\n)\n\n# Mixed precision scaler\nscaler = torch.cuda.amp.GradScaler()\n\nprint(\"Training setup:\")\nprint(f\"  Epochs: {NUM_EPOCHS}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION}\")\nprint(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\nprint(f\"  Learning rate: {LEARNING_RATE}\")\nprint(f\"  Training steps: {num_training_steps}\")\nprint(f\"  Warmup steps: {min(WARMUP_STEPS, num_training_steps // 10)}\")\nprint(f\"  Train batches/epoch: {len(train_dataloader)}\")\nprint(f\"  Val batches: {len(val_dataloader)}\")\n\n# Validation function\n@torch.no_grad()\ndef validate(model, val_dataloader, device):\n    model.eval()\n    total_loss = 0.0\n    num_batches = 0\n    \n    for batch in val_dataloader:\n        with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n            loss = training_step(model, batch, device)\n        \n        # Skip NaN losses in validation too\n        if not torch.isnan(loss) and not torch.isinf(loss):\n            total_loss += loss.item()\n            num_batches += 1\n    \n    model.train()\n    \n    if num_batches == 0:\n        return float('nan')\n    \n    return total_loss / num_batches"
  },
  {
   "cell_type": "code",
   "source": "# Debug: Test a single training step to diagnose NaN issues\nprint(\"Running diagnostic training step...\")\n\n# Get a single batch\ntest_batch = next(iter(train_dataloader))\n\n# Check batch data\nprint(\"\\nBatch data statistics:\")\nprint(f\"  Latent shape: {test_batch['latent'].shape}\")\nprint(f\"  Latent range: [{test_batch['latent'].min():.4f}, {test_batch['latent'].max():.4f}]\")\nprint(f\"  Latent has NaN: {torch.isnan(test_batch['latent']).any()}\")\nprint(f\"  Speaker latent shape: {test_batch['speaker_latent'].shape}\")\nprint(f\"  Speaker latent range: [{test_batch['speaker_latent'].min():.4f}, {test_batch['speaker_latent'].max():.4f}]\")\nprint(f\"  Speaker latent has NaN: {torch.isnan(test_batch['speaker_latent']).any()}\")\nprint(f\"  Texts: {test_batch['text']}\")\n\n# Try forward pass\ntry:\n    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n        loss = training_step(model, test_batch, DEVICE)\n    print(f\"\\nTest loss: {loss.item():.4f}\")\n    print(\"✓ Forward pass successful!\")\n    \n    # Try backward pass\n    loss.backward()\n    print(\"✓ Backward pass successful!\")\n    \n    # Check gradients\n    grad_norms = []\n    for name, param in model.named_parameters():\n        if param.grad is not None:\n            grad_norm = param.grad.norm().item()\n            grad_norms.append((name, grad_norm))\n            if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n                print(f\"  ⚠️  NaN/Inf gradient in {name}\")\n    \n    if grad_norms:\n        # Sort by gradient magnitude\n        grad_norms.sort(key=lambda x: x[1], reverse=True)\n        print(f\"\\nTop 5 gradient magnitudes:\")\n        for name, norm in grad_norms[:5]:\n            print(f\"  {name}: {norm:.4f}\")\n    \n    optimizer.zero_grad()\n    \nexcept Exception as e:\n    print(f\"\\n❌ Error during test step: {e}\")\n    import traceback\n    traceback.print_exc()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training loop with validation\nprint(\"\\n\" + \"=\"*50)\nprint(\"Starting training...\")\nprint(\"=\"*50 + \"\\n\")\n\nhistory = {\"train_loss\": [], \"val_loss\": [], \"epoch\": [], \"lr\": []}\nbest_val_loss = float(\"inf\")\n\nfor epoch in range(NUM_EPOCHS):\n    # Train one epoch (scheduler is now passed and called inside train_epoch)\n    train_loss = train_epoch(\n        model=model,\n        dataloader=train_dataloader,\n        optimizer=optimizer,\n        scheduler=scheduler,  # Pass scheduler to training loop\n        device=DEVICE,\n        gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n        max_grad_norm=MAX_GRAD_NORM,\n        scaler=scaler,\n    )\n    \n    # Validate\n    val_loss = validate(model, val_dataloader, DEVICE)\n    \n    # Get current LR (scheduler is stepped inside train_epoch now)\n    current_lr = scheduler.get_last_lr()[0]\n    \n    # Record history\n    history[\"train_loss\"].append(train_loss)\n    history[\"val_loss\"].append(val_loss)\n    history[\"epoch\"].append(epoch + 1)\n    history[\"lr\"].append(current_lr)\n    \n    # Print progress\n    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} - Train: {train_loss:.4f} - Val: {val_loss:.4f} - LR: {current_lr:.2e}\")\n    \n    # Save best checkpoint (based on validation loss)\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        save_lora_checkpoint(\n            model,\n            os.path.join(OUTPUT_DIR, \"lora_best.pt\"),\n            config={\n                \"rank\": LORA_RANK,\n                \"alpha\": LORA_ALPHA,\n                \"dropout\": LORA_DROPOUT,\n                \"target_modules\": TARGET_MODULES,\n                \"epoch\": epoch + 1,\n                \"train_loss\": train_loss,\n                \"val_loss\": val_loss,\n            }\n        )\n        print(f\"  -> Saved best checkpoint (val_loss: {best_val_loss:.4f})\")\n    \n    # Periodic checkpoint every 2 epochs\n    if (epoch + 1) % 2 == 0:\n        save_lora_checkpoint(\n            model,\n            os.path.join(OUTPUT_DIR, f\"lora_epoch_{epoch + 1}.pt\"),\n            config={\n                \"rank\": LORA_RANK,\n                \"alpha\": LORA_ALPHA,\n                \"dropout\": LORA_DROPOUT,\n                \"target_modules\": TARGET_MODULES,\n                \"epoch\": epoch + 1,\n                \"train_loss\": train_loss,\n                \"val_loss\": val_loss,\n            }\n        )\n\n# Save final checkpoint\nsave_lora_checkpoint(\n    model,\n    os.path.join(OUTPUT_DIR, \"lora_final.pt\"),\n    config={\n        \"rank\": LORA_RANK,\n        \"alpha\": LORA_ALPHA,\n        \"dropout\": LORA_DROPOUT,\n        \"target_modules\": TARGET_MODULES,\n        \"epoch\": NUM_EPOCHS,\n        \"train_loss\": history[\"train_loss\"][-1],\n        \"val_loss\": history[\"val_loss\"][-1],\n    }\n)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Training complete!\")\nprint(f\"Best validation loss: {best_val_loss:.4f}\")\nprint(f\"Checkpoints saved to: {OUTPUT_DIR}\")\nprint(\"=\"*50)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Training and validation loss\n",
    "axes[0].plot(history[\"epoch\"], history[\"train_loss\"], 'b-', linewidth=2, label='Train')\n",
    "axes[0].plot(history[\"epoch\"], history[\"val_loss\"], 'r--', linewidth=2, label='Val')\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"Training vs Validation Loss\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Training loss only (zoomed)\n",
    "axes[1].plot(history[\"epoch\"], history[\"train_loss\"], 'b-', linewidth=2)\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Loss\")\n",
    "axes[1].set_title(\"Training Loss\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "axes[2].plot(history[\"epoch\"], history[\"lr\"], 'g-', linewidth=2)\n",
    "axes[2].set_xlabel(\"Epoch\")\n",
    "axes[2].set_ylabel(\"Learning Rate\")\n",
    "axes[2].set_title(\"Learning Rate Schedule\")\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"training_curves.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Final train loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"  Final val loss: {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"  Best val loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation & Inference\n",
    "\n",
    "Generate samples with your fine-tuned model and compare to the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Test prompts - rap style!\n",
    "TEST_PROMPTS = [\n",
    "    \"[S1] Yeah, I'm spitting fire on the mic tonight, gonna show them what I got, rising to the top!\",\n",
    "    \"[S1] The rhythm flows through me like water, every beat hits harder, I'm a natural born starter!\",\n",
    "    \"[S1] Check it out, I'm the one they've been waiting for, coming through the door, ready to explore!\",\n",
    "    \"[S1] Money on my mind, grind never stops, from the bottom to the top, watch me drop!\",\n",
    "    \"[S1] Real recognize real, that's the deal, keep it trill, got the skill to make you feel!\",\n",
    "]\n",
    "\n",
    "# Use a random training file as speaker reference\n",
    "SPEAKER_AUDIO_PATH = random.choice(audio_files) if audio_files else None\n",
    "\n",
    "print(f\"Using speaker reference: {SPEAKER_AUDIO_PATH}\")\n",
    "print(f\"\\nWill generate {len(TEST_PROMPTS)} samples...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples with fine-tuned model\n",
    "@torch.inference_mode()\n",
    "def generate_audio(model, text, speaker_audio_path=None, seed=0):\n",
    "    \"\"\"Generate audio using the (fine-tuned) model.\"\"\"\n",
    "    \n",
    "    # Load speaker audio\n",
    "    if speaker_audio_path:\n",
    "        speaker_audio = load_audio(speaker_audio_path)\n",
    "    else:\n",
    "        speaker_audio = None\n",
    "    \n",
    "    # Create sample function\n",
    "    sample_fn = partial(\n",
    "        sample_euler_cfg_independent_guidances,\n",
    "        num_steps=40,\n",
    "        cfg_scale_text=3.0,\n",
    "        cfg_scale_speaker=8.0,\n",
    "        cfg_min_t=0.5,\n",
    "        cfg_max_t=1.0,\n",
    "        truncation_factor=0.8,\n",
    "        rescale_k=None,\n",
    "        rescale_sigma=None,\n",
    "        speaker_kv_scale=None,\n",
    "        speaker_kv_max_layers=None,\n",
    "        speaker_kv_min_t=None,\n",
    "        sequence_length=640,\n",
    "    )\n",
    "    \n",
    "    # Generate\n",
    "    audio_out, normalized_text = sample_pipeline(\n",
    "        model=model,\n",
    "        fish_ae=fish_ae,\n",
    "        pca_state=pca_state,\n",
    "        sample_fn=sample_fn,\n",
    "        text_prompt=text,\n",
    "        speaker_audio=speaker_audio,\n",
    "        rng_seed=seed,\n",
    "    )\n",
    "    \n",
    "    return audio_out[0].cpu(), normalized_text\n",
    "\n",
    "# Generate and play samples\n",
    "print(\"Generating samples with fine-tuned model...\\n\")\n",
    "\n",
    "for i, prompt in enumerate(TEST_PROMPTS):\n",
    "    print(f\"Prompt {i + 1}: {prompt}\")\n",
    "    \n",
    "    audio, _ = generate_audio(\n",
    "        model,\n",
    "        prompt,\n",
    "        speaker_audio_path=str(SPEAKER_AUDIO_PATH) if SPEAKER_AUDIO_PATH else None,\n",
    "        seed=i,\n",
    "    )\n",
    "    \n",
    "    # Save audio\n",
    "    output_path = os.path.join(OUTPUT_DIR, f\"sample_{i + 1}.wav\")\n",
    "    torchaudio.save(output_path, audio.unsqueeze(0), 44100)\n",
    "    print(f\"Saved to: {output_path}\")\n",
    "    \n",
    "    # Play audio\n",
    "    display(Audio(audio.numpy(), rate=44100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Checkpoint for Later Use\n",
    "\n",
    "Use this section to load a saved LoRA checkpoint onto a fresh model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load a saved LoRA checkpoint\n",
    "# Uncomment and run this cell to load a checkpoint\n",
    "\n",
    "# CHECKPOINT_PATH = os.path.join(OUTPUT_DIR, \"lora_best.pt\")\n",
    "# \n",
    "# # Load fresh base model\n",
    "# model_fresh = load_model_from_hf(\n",
    "#     device=DEVICE,\n",
    "#     dtype=DTYPE,\n",
    "#     compile=False,\n",
    "#     delete_blockwise_modules=True,\n",
    "# )\n",
    "# \n",
    "# # Load checkpoint to get config\n",
    "# checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "# config = checkpoint[\"config\"]\n",
    "# \n",
    "# # Apply LoRA with saved config\n",
    "# model_fresh, _ = apply_lora_to_model(\n",
    "#     model_fresh,\n",
    "#     rank=config[\"rank\"],\n",
    "#     alpha=config[\"alpha\"],\n",
    "#     dropout=0.0,  # No dropout for inference\n",
    "#     target_modules=config[\"target_modules\"],\n",
    "# )\n",
    "# \n",
    "# # Load LoRA weights\n",
    "# load_lora_checkpoint(model_fresh, CHECKPOINT_PATH, device=DEVICE)\n",
    "# model_fresh.eval()\n",
    "# \n",
    "# print(f\"Loaded checkpoint from epoch {config['epoch']} (loss: {config['loss']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Tips & Notes for Rap Training\n",
    "\n",
    "### What We Configured for 225 Samples\n",
    "- **LoRA rank 32**: Higher rank captures more style nuance with larger dataset\n",
    "- **Lower dropout (0.05)**: Less regularization needed with more data\n",
    "- **Higher learning rate (1e-4)**: Can train faster with more data\n",
    "- **Fewer epochs (10)**: More data means fewer passes needed\n",
    "- **Whisper medium**: Better accuracy for rap lyrics than base model\n",
    "\n",
    "### Training Expectations\n",
    "With 225 rap acapellas:\n",
    "- **Training time**: ~2-4 hours on T4, ~1-2 hours on A100\n",
    "- **Expected final loss**: ~0.05-0.15 (lower is better)\n",
    "- **Checkpoint size**: ~80-100MB\n",
    "\n",
    "### If Results Sound Off\n",
    "1. **Too monotone**: Increase `cfg_scale_text` to 4-5 during inference\n",
    "2. **Wrong rhythm**: The model learned general rap style, not specific flows\n",
    "3. **Voice doesn't match**: Try different speaker reference audio\n",
    "4. **Gibberish output**: Check if transcriptions were accurate\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "**Out of Memory (OOM)**:\n",
    "- Reduce `MAX_LATENT_LENGTH` to 320 (15 seconds max)\n",
    "- Use A100 GPU on Colab instead of T4\n",
    "- Reduce `LORA_RANK` to 16\n",
    "\n",
    "**Loss not decreasing**:\n",
    "- Check transcriptions are accurate (rap lyrics are hard!)\n",
    "- Try `WHISPER_MODEL = \"large-v3\"` for better transcription\n",
    "\n",
    "**Validation loss increasing (overfitting)**:\n",
    "- Increase `LORA_DROPOUT` to 0.1\n",
    "- Reduce `NUM_EPOCHS`\n",
    "- Reduce `LORA_RANK` to 16\n",
    "\n",
    "### Voice Cloning Still Works!\n",
    "The speaker path (wk_speaker, wv_speaker) was kept frozen, so you can:\n",
    "- Use ANY speaker reference audio at inference time\n",
    "- The rap style transfers to any voice\n",
    "- Original voice cloning quality is preserved"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}